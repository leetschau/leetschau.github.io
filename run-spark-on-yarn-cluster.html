<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Li Chao" />
        <meta name="copyright" content="Li Chao" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="spark, yarn, hadoop, Tech, " />

<meta property="og:title" content="Run Spark on Yarn Cluster "/>
<meta property="og:url" content="http://leetschau.github.io/run-spark-on-yarn-cluster.html" />
<meta property="og:description" content="Run WordCount on 3-Node Yarn Cluster According to How to Install and Set Up a 3-Node Hadoop Cluster. Create 3 Ubuntu 16.04 hosts with vagrant on Lenovo ThinkCenter. Configure the network, hostname and IP address in each Vagrantfile. The memory should be no less than 8GB for each host …" />
<meta property="og:site_name" content="DarkMatter in Cyberspace" />
<meta property="og:article:author" content="Li Chao" />
<meta property="og:article:published_time" content="2018-02-13T16:00:04+08:00" />
<meta property="" content="2018-02-13T16:04:15+08:00" />
<meta name="twitter:title" content="Run Spark on Yarn Cluster ">
<meta name="twitter:description" content="Run WordCount on 3-Node Yarn Cluster According to How to Install and Set Up a 3-Node Hadoop Cluster. Create 3 Ubuntu 16.04 hosts with vagrant on Lenovo ThinkCenter. Configure the network, hostname and IP address in each Vagrantfile. The memory should be no less than 8GB for each host …">

        <title>Run Spark on Yarn Cluster  · DarkMatter in Cyberspace
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/css/custom.css" media="screen">
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="http://leetschau.github.io/"><span class=site-name>DarkMatter in Cyberspace</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="http://leetschau.github.io">Home</a></li>
                            <li ><a href="http://leetschau.github.io/categories.html">Categories</a></li>
                            <li ><a href="http://leetschau.github.io/tags.html">Tags</a></li>
                            <li ><a href="http://leetschau.github.io/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="http://leetschau.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="http://leetschau.github.io/run-spark-on-yarn-cluster.html"> Run Spark on Yarn Cluster  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            
            <hr>
<h1>Run WordCount on 3-Node Yarn Cluster</h1>
<p>According to <a href="https://linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/">How to Install and Set Up a 3-Node Hadoop Cluster</a>.</p>
<p>Create 3 Ubuntu 16.04 hosts with vagrant on Lenovo ThinkCenter.
Configure the network, hostname and IP address in each Vagrantfile.
The memory should be no less than 8GB for each host.
Or you have to configure yarn's memory usage manually.
The node-master Vagrantfile:</p>
<div class="highlight"><pre><span></span># <span class="o">-*-</span> <span class="nv">mode</span>: <span class="nv">ruby</span> <span class="o">-*-</span>
<span class="nv">Vagrant</span>.<span class="nv">configure</span><span class="ss">(</span><span class="mi">2</span><span class="ss">)</span> <span class="k">do</span> <span class="o">|</span><span class="nv">config</span><span class="o">|</span>
  <span class="nv">config</span>.<span class="nv">vm</span>.<span class="nv">box</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="s">ubuntu/xenial64</span><span class="s2">&quot;</span>
  <span class="nv">config</span>.<span class="nv">ssh</span>.<span class="nv">username</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="s">ubuntu</span><span class="s2">&quot;</span>
  <span class="nv">config</span>.<span class="nv">ssh</span>.<span class="nv">password</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="s">3d7d18ebe09a49ff99028120</span><span class="s2">&quot;</span>
  <span class="nv">config</span>.<span class="nv">vm</span>.<span class="nv">define</span> <span class="s2">&quot;</span><span class="s">yarnmaster</span><span class="s2">&quot;</span>
  <span class="nv">config</span>.<span class="nv">vm</span>.<span class="nv">hostname</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="s">nodemaster</span><span class="s2">&quot;</span>
  <span class="nv">config</span>.<span class="nv">vm</span>.<span class="nv">network</span> <span class="s2">&quot;</span><span class="s">private_network</span><span class="s2">&quot;</span>, <span class="nv">ip</span>: <span class="s2">&quot;</span><span class="s">192.0.2.1</span><span class="s2">&quot;</span>
  <span class="nv">config</span>.<span class="nv">vm</span>.<span class="nv">provider</span> <span class="s2">&quot;</span><span class="s">virtualbox</span><span class="s2">&quot;</span> <span class="k">do</span> <span class="o">|</span><span class="nv">vb</span><span class="o">|</span>
    <span class="nv">vb</span>.<span class="nv">memory</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="s">8192</span><span class="s2">&quot;</span>
  <span class="k">end</span>
<span class="k">end</span>
</pre></div>


<p>Add the following lines into <em>/etc/hosts</em> of each host:</p>
<div class="highlight"><pre><span></span><span class="mi">192</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="mi">2</span><span class="p">.</span><span class="mi">1</span>    <span class="n">node</span><span class="o">-</span><span class="n">master</span>
<span class="mi">192</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="mi">2</span><span class="p">.</span><span class="mi">2</span>    <span class="n">node1</span>
<span class="mi">192</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="mi">2</span><span class="p">.</span><span class="mi">3</span>    <span class="n">node2</span>
</pre></div>


<p>Install JDK on each host:</p>
<div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="n">apt</span> <span class="k">update</span>
<span class="n">sudo</span> <span class="n">apt</span> <span class="n">install</span> <span class="o">-</span><span class="n">y</span> <span class="n">openjdk</span><span class="o">-</span><span class="mi">8</span><span class="o">-</span><span class="n">jdk</span>
</pre></div>


<p>Create <code>hadoop</code> user on each host:</p>
<div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="n">useradd</span> <span class="o">-</span><span class="n">m</span> <span class="n">hadoop</span>
<span class="n">sudo</span> <span class="n">passwd</span> <span class="n">hadoop</span>   <span class="o">#</span> <span class="k">input</span> <span class="n">password</span><span class="p">:</span> <span class="n">hadoop</span>
</pre></div>


<p>Configure SSH access: on node-master:</p>
<div class="highlight"><pre><span></span><span class="n">sudo</span><span class="w"> </span><span class="n">su</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">hadoop</span><span class="w"></span>
<span class="n">ssh</span><span class="o">-</span><span class="n">keygen</span><span class="w"></span>
<span class="n">ssh</span><span class="o">-</span><span class="n">copy</span><span class="o">-</span><span class="n">id</span><span class="w"> </span><span class="n">hadoop</span><span class="nv">@node</span><span class="o">-</span><span class="n">master</span><span class="w"></span>
<span class="n">ssh</span><span class="o">-</span><span class="n">copy</span><span class="o">-</span><span class="n">id</span><span class="w"> </span><span class="n">hadoop</span><span class="nv">@node1</span><span class="w"></span>
<span class="n">ssh</span><span class="o">-</span><span class="n">copy</span><span class="o">-</span><span class="n">id</span><span class="w"> </span><span class="n">hadoop</span><span class="nv">@node2</span><span class="w"></span>
</pre></div>


<p>Install Hadoop as user <em>hadoop</em> on node-master and duplicate to other nodes:</p>
<div class="highlight"><pre><span></span><span class="n">wget</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">mirror</span><span class="p">.</span><span class="nb">bit</span><span class="p">.</span><span class="n">edu</span><span class="p">.</span><span class="n">cn</span><span class="o">/</span><span class="n">apache</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">common</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">9</span><span class="p">.</span><span class="mi">0</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">9</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="n">tar</span><span class="p">.</span><span class="n">gz</span>
<span class="n">tar</span> <span class="n">xf</span> <span class="n">hadoop</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">9</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="n">tar</span><span class="p">.</span><span class="n">gz</span>
<span class="n">mv</span> <span class="n">hadoop</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">9</span><span class="p">.</span><span class="mi">0</span> <span class="n">hadoop</span>
<span class="n">echo</span> <span class="s1">&#39;PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:$PATH&#39;</span> <span class="o">&gt;&gt;</span> <span class="err">$</span><span class="n">HOME</span><span class="o">/</span><span class="p">.</span><span class="n">profile</span>
</pre></div>


<p>Modify files under <em>~/hadoop/etc/hadoop/</em> and duplicate the files on each node:</p>
<div class="highlight"><pre><span></span><span class="n">scp</span> <span class="o">-</span><span class="n">r</span> <span class="n">hadoop</span> <span class="n">node1</span><span class="p">:</span><span class="o">~/</span>
<span class="n">scp</span> <span class="o">-</span><span class="n">r</span> <span class="n">hadoop</span> <span class="n">node2</span><span class="p">:</span><span class="o">~/</span>
<span class="n">scp</span> <span class="p">.</span><span class="n">profile</span> <span class="n">node1</span><span class="p">:</span><span class="o">~/</span>
<span class="n">scp</span> <span class="p">.</span><span class="n">profile</span> <span class="n">node2</span><span class="p">:</span><span class="o">~/</span>
</pre></div>


<p>Note 1: Spark 2.2.1 using Hadoop 2.7+, so we download Hadoop 2.9.0.</p>
<p>Note 2: The memory of each host is 8GB. So I didn't modify the memory configurations.</p>
<p>Start HDFS: on node-master, run:</p>
<div class="highlight"><pre><span></span><span class="nt">hdfs</span> <span class="nt">namenode</span> <span class="nt">-format</span>
<span class="nt">start-dfs</span><span class="p">.</span><span class="nc">sh</span>
<span class="nt">jps</span>   <span class="err">#</span> <span class="nt">on</span> <span class="nt">node-master</span><span class="o">:</span> <span class="nt">NameNode</span> <span class="nt">and</span> <span class="nt">SecondaryNameNode</span><span class="o">;</span> <span class="nt">on</span> <span class="nt">node1</span><span class="o">/</span><span class="nt">2</span><span class="o">:</span> <span class="nt">DataNode</span>
<span class="nt">hdfs</span> <span class="nt">dfsadmin</span> <span class="nt">-report</span>
<span class="nt">hdfs</span> <span class="nt">dfs</span> <span class="nt">-mkdir</span> <span class="nt">-p</span> <span class="o">/</span><span class="nt">user</span><span class="o">/</span><span class="nt">hadoop</span>
<span class="nt">wget</span> <span class="nt">-O</span> <span class="nt">alice</span><span class="p">.</span><span class="nc">txt</span> <span class="nt">https</span><span class="o">://</span><span class="nt">www</span><span class="p">.</span><span class="nc">gutenberg</span><span class="p">.</span><span class="nc">org</span><span class="o">/</span><span class="nt">files</span><span class="o">/</span><span class="nt">11</span><span class="o">/</span><span class="nt">11-0</span><span class="p">.</span><span class="nc">txt</span>
<span class="nt">wget</span> <span class="nt">-O</span> <span class="nt">holmes</span><span class="p">.</span><span class="nc">txt</span> <span class="nt">https</span><span class="o">://</span><span class="nt">www</span><span class="p">.</span><span class="nc">gutenberg</span><span class="p">.</span><span class="nc">org</span><span class="o">/</span><span class="nt">ebooks</span><span class="o">/</span><span class="nt">1661</span><span class="p">.</span><span class="nc">txt</span><span class="p">.</span><span class="nc">utf-8</span>
<span class="nt">wget</span> <span class="nt">-O</span> <span class="nt">frankenstein</span><span class="p">.</span><span class="nc">txt</span> <span class="nt">https</span><span class="o">://</span><span class="nt">www</span><span class="p">.</span><span class="nc">gutenberg</span><span class="p">.</span><span class="nc">org</span><span class="o">/</span><span class="nt">ebooks</span><span class="o">/</span><span class="nt">84</span><span class="p">.</span><span class="nc">txt</span><span class="p">.</span><span class="nc">utf-8</span>
<span class="nt">hdfs</span> <span class="nt">dfs</span> <span class="nt">-mkdir</span> <span class="nt">books</span>
<span class="nt">hdfs</span> <span class="nt">dfs</span> <span class="nt">-put</span> <span class="nt">alice</span><span class="p">.</span><span class="nc">txt</span> <span class="nt">holmes</span><span class="p">.</span><span class="nc">txt</span> <span class="nt">frankenstein</span><span class="p">.</span><span class="nc">txt</span> <span class="nt">books</span>
<span class="err">#</span> <span class="nt">on</span> <span class="nt">node1</span><span class="o">:</span>
<span class="nt">hdfs</span> <span class="nt">dfs</span> <span class="nt">-get</span> <span class="nt">books</span><span class="o">/</span><span class="nt">alice</span><span class="p">.</span><span class="nc">txt</span>
</pre></div>


<p>Start Yarn and run a <em>wordcount</em> app:</p>
<div class="highlight"><pre><span></span><span class="nt">start-yarn</span><span class="p">.</span><span class="nc">sh</span>
<span class="nt">jps</span>    <span class="err">#</span> <span class="nt">on</span> <span class="nt">node-master</span><span class="o">:</span> <span class="nt">ResourceManager</span><span class="o">;</span> <span class="nt">on</span> <span class="nt">node1</span><span class="o">/</span><span class="nt">2</span><span class="o">:</span> <span class="nt">NodeManager</span>
<span class="nt">yarn</span> <span class="nt">node</span> <span class="nt">-list</span>     <span class="err">#</span> <span class="nt">this</span> <span class="nt">works</span> <span class="nt">on</span> <span class="nt">both</span> <span class="nt">node-master</span> <span class="nt">and</span> <span class="nt">node1</span><span class="o">/</span><span class="nt">2</span>
<span class="nt">yarn</span> <span class="nt">application</span> <span class="nt">-list</span>
<span class="nt">yarn</span> <span class="nt">jar</span> <span class="o">~/</span><span class="nt">hadoop</span><span class="o">/</span><span class="nt">share</span><span class="o">/</span><span class="nt">hadoop</span><span class="o">/</span><span class="nt">mapreduce</span><span class="o">/</span><span class="nt">hadoop-mapreduce-examples-2</span><span class="p">.</span><span class="nc">9</span><span class="p">.</span><span class="nc">0</span><span class="p">.</span><span class="nc">jar</span> <span class="nt">wordcount</span> <span class="s2">&quot;books/*&quot;</span> <span class="nt">output</span>
<span class="nt">hdfs</span> <span class="nt">dfs</span> <span class="nt">-ls</span> <span class="nt">output</span>
</pre></div>


<h1>Run Spark on Yarn</h1>
<p>According to <a href="https://linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/">Install, Configure, and Run Spark on Top of a Hadoop YARN Cluster</a>.</p>
<p>Download and install Spark on each host (as user <strong>hadoop</strong>):</p>
<div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">hadoop</span>
<span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">d3kbcqa49mib13</span><span class="p">.</span><span class="n">cloudfront</span><span class="p">.</span><span class="n">net</span><span class="o">/</span><span class="n">spark</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">2</span><span class="p">.</span><span class="mi">0</span><span class="o">-</span><span class="n">bin</span><span class="o">-</span><span class="n">hadoop2</span><span class="p">.</span><span class="mi">7</span><span class="p">.</span><span class="n">tgz</span>
<span class="n">tar</span> <span class="o">-</span><span class="n">xvf</span> <span class="n">spark</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">2</span><span class="p">.</span><span class="mi">0</span><span class="o">-</span><span class="n">bin</span><span class="o">-</span><span class="n">hadoop2</span><span class="p">.</span><span class="mi">7</span><span class="p">.</span><span class="n">tgz</span>
<span class="n">mv</span> <span class="n">spark</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">2</span><span class="p">.</span><span class="mi">0</span><span class="o">-</span><span class="n">bin</span><span class="o">-</span><span class="n">hadoop2</span><span class="p">.</span><span class="mi">7</span> <span class="n">spark</span>
</pre></div>


<p>Edit <em>~/.profile</em>:</p>
<div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">PATH</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">spark</span><span class="o">/</span><span class="n">bin</span><span class="p">:</span><span class="err">$</span><span class="n">PATH</span>
<span class="n">export</span> <span class="n">HADOOP_CONF_DIR</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">hadoop</span>
<span class="n">export</span> <span class="n">SPARK_HOME</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">spark</span>
<span class="n">export</span> <span class="n">LD_LIBRARY_PATH</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">native</span><span class="p">:</span><span class="err">$</span><span class="n">LD_LIBRARY_PATH</span>
</pre></div>


<p>Run <code>spark-shell</code> on node-master, some errors raised.
In the spark-shell, the <code>spark</code> variable doesn't exists.</p>
            
            
            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2018-02-13T16:00:04+08:00">Feb 13, 2018</time>

<h4>Last Updated</h4>
<time datetime="2018-02-13T16:04:15+08:00">Feb 13, 2018</time>

            <h4>Category</h4>
            <a class="category-link" href="http://leetschau.github.io/categories.html#tech-ref">Tech</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="http://leetschau.github.io/tags.html#hadoop-ref">hadoop
                    <span>7</span>
</a></li>
                <li><a href="http://leetschau.github.io/tags.html#spark-ref">spark
                    <span>21</span>
</a></li>
                <li><a href="http://leetschau.github.io/tags.html#yarn-ref">yarn
                    <span>2</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="#" title="My You can add links in your config file Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-you can add links in your config file sidebar-social-links"></i></a>
    <a href="#" title="My Another social link Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-another social link sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>