<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Dark Matter in Cyberspace]]></title>
  <link href="http://leetschau.github.io/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://leetschau.github.io/"/>
  <updated>2016-02-21T11:06:40+08:00</updated>
  <id>http://leetschau.github.io/</id>
  <author>
    <name><![CDATA[Li Chao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Compare Users in 2 Days With Spark]]></title>
    <link href="http://leetschau.github.io/blog/2015/01/04/173843/"/>
    <updated>2015-01-04T17:38:43+08:00</updated>
    <id>http://leetschau.github.io/blog/2015/01/04/173843</id>
    <content type="html"><![CDATA[<p>We have 2 log files. One contains logs created in 2014.11.22, the other all in 2014.11.23.
Each have about 240 million logs in it, with file size 30GB.</p>

<p>The log in these files are like this:</p>

<blockquote><p>&ldquo;460015482006002&rdquo;,&ldquo;深圳市&rdquo;,&ldquo;广东省&rdquo;,&ldquo;2014-12-22  16:46:04&rdquo;,&ldquo;2014-12-22  16:46:04&rdquo;,&ldquo;42705&rdquo;,&ldquo;16111&rdquo;,&ldquo;460014270516111&rdquo;,&ldquo;MAP&rdquo;</p></blockquote>

<p>The number in the first column &ldquo;460015482006002&rdquo; represents a unique user, which is called &ldquo;IMSI&rdquo;.
We need to find all IMSI number <strong>only</strong> exists in 2014.11.22.</p>

<p>Given the huge size of log files, we compute with Apache Spark.</p>

<p>Create only22.scala:</p>

<pre><code>import java.util.Date
import java.util.Calendar
import java.util.concurrent.TimeUnit

val SIG_DATA1 = "datamine/drawdata_20141122_cs.utf8.csv"
val SIG_DATA2 = "datamine/drawdata_20141123_cs.utf8.csv"
val USER_ID_POS = 1

def get_users(data: String): Set[String] = {
  val raw_data = sc.textFile(data).map(_.split("\"").toList)
  val raw_sig_map = raw_data.filter(x =&gt; x.size &gt; 17)
  val users = raw_sig_map.map(_(USER_ID_POS)).distinct
  return users.toArray.toSet
}

val u1 = get_users(SIG_DATA1)
val u2 = get_users(SIG_DATA2)
val only22 = u1 -- u2
scala.tools.nsc.io.File("only22.txt").writeAll(only22.toString)
</code></pre>

<p>Run it: <code>spark-shell --master spark://cloud142:7077 --driver-memory 6g --driver-cores 5 --total-executor-cores 28 --executor-memory 20g -i only22.scala</code></p>

<p>In file only22.txt, all elements in the set are written in one line.
So we have to replace all &ldquo;,&rdquo; with newline character: <code>sed -i 's/,/\n/g' only22.txt</code>;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark任务自动执行脚本]]></title>
    <link href="http://leetschau.github.io/blog/2014/11/03/171108/"/>
    <updated>2014-11-03T17:11:08+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/11/03/171108</id>
    <content type="html"><![CDATA[<p>当前目录下创建两个脚本，运行脚本runJobs.sh和WFP任务脚本模板wfp-origin：</p>

<p>runJobs:</p>

<pre><code>#!/bin/bash

BMIN_CNT_WEI_LIST='0.5 0.3'
MIN_SUP_LIST='0.004 0.0002'

rm -rf {result,script}
mkdir {result,script}
origin_job=wfp-origin
for bmin_cnt_wei in $BMIN_CNT_WEI_LIST; do
    for min_sup in $MIN_SUP_LIST; do
        echo MIN_SUP is: $min_sup , Bmin count weight is: $bmin_cnt_wei
        new_script_name=wfp-bmin-${bmin_cnt_wei}-sup-${min_sup}
        sed "4s/xxx/$min_sup/" $origin_job &gt; script/tmp
        sed "11s/xxx/$bmin_cnt_wei/" script/tmp &gt; script/$new_script_name
        rm script/tmp
        echo Run shell script/$new_script_name
        spark-shell -i script/$new_script_name
        echo ------Calc is over------
    done
done
</code></pre>

<p>wfp-origin:</p>

<pre><code>import Math.ceil
import scala.io

val MIN_SUP = xxx
val MIN_CONF = 0.1
val MAX_RELATION_ORDER = 3
val DATA_FILE = "input"
val SEP = "\001"
val WEI_IDX = 5
val MIN_INT_ID_LEN = 5
val BMIN_CNT_WEI = xxx

val RES_FILE = "result/wfp-result-bmin-" + BMIN_CNT_WEI.toString + "-sup-" + MIN_SUP.toString

val rawData = sc.textFile(DATA_FILE).distinct
val data = rawData.filter(x =&gt; x.split(SEP)(1).split("_")(0).size &gt; MIN_INT_ID_LEN)   
val item_count = data.map(_.split(SEP)(1)).map(w =&gt; (w,1)).reduceByKey(_+_)
val wids = data.map(_.split(SEP)(0))
val MAX_ITEM = wids.map(w =&gt; (w,1)).reduceByKey(_+_).map(_._2).max
val T = wids.distinct.count

val weight = data.map(x =&gt; (x.split(SEP)(1), x.split(SEP)(WEI_IDX).toFloat)).distinct
val rule_sets = weight.top(2000)
scala.tools.nsc.io.File(RES_FILE).writeAll(rule_sets.map(_.toString).reduce(_ + "\n" + _))
exit
</code></pre>

<p>这里的wfp-orgin只是一个示例，根据自己的脚本调整具体内容，但最后一行"exit"必须有，否则不能退出spark shell执行后面的任务。</p>

<p>需要修改的参数用空格分开定义在一个字符串里，例如上面的BMIN_CNT_WEI_LIST和MIN_SUP_LIST，
脚本创建script目录，保存新生成的脚本，以及result目录，保存计算结果。</p>

<p>配合tmux，可以在tmux上运行runJobs.sh，下班时从tmux上detach出来，第二天上班时在attach上去看结果。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Spark Job Runner in Web Browser]]></title>
    <link href="http://leetschau.github.io/blog/2014/10/27/111204/"/>
    <updated>2014-10-27T11:12:04+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/10/27/111204</id>
    <content type="html"><![CDATA[<p>I wrote a Spark script. Now I need adding a web interface for it. Thus users can input parameters of the script in web browser, run the script,
and see the calculation results in browser.</p>

<h1>JEE</h1>

<p>Download apache tomcat 7.0.56 core (apache-tomcat-7.0.56.tar.gz) and extract it to ~/apps;</p>

<p>Download Eclipse IDE for Java EE Developers (eclipse-jee-luna-SR1-linux-gtk-x86_64.tar.gz) and extract it to ~/apps;</p>

<p>Start Eclipse IDE for Java EE Developers, build a new Web project: [File -> New -> Web -> Dynamic Web Project];</p>

<p>Specify &ldquo;Project&rdquo; as &ldquo;SparkRunner&rdquo;, &ldquo;Target runtime&rdquo; as &ldquo;Apache Tomcat v7.0&rdquo;.
If the default runtime has not existed yet, build a new one: Type of runtime environment: Apache -> Apache Tomcat v7.0).
Check &ldquo;Generate web.xml deployment descriptor&rdquo; at the last step and click &ldquo;Finish&rdquo;.</p>

<p>Add the following texts into &ldquo;web-app&rdquo; of web.xml under $PROJECT_HOME/WebContent/WEB-INF:</p>

<pre><code>&lt;servlet&gt;
    &lt;servlet-name&gt;JobRunner&lt;/servlet-name&gt;
    &lt;servlet-class&gt;com.boco.dm.JobRunner&lt;/servlet-class&gt;
&lt;/servlet&gt;
&lt;servlet-mapping&gt;
    &lt;servlet-name&gt;JobRunner&lt;/servlet-name&gt;
    &lt;url-pattern&gt;/JobRunner&lt;/url-pattern&gt;
&lt;/servlet-mapping&gt;
</code></pre>

<p>Ref:</p>

<p><a href="https://cloud.google.com/appengine/docs/java/config/webxml#About_Deployment_Descriptors">The Deployment Descriptor: web.xml</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[告警资源过滤算法]]></title>
    <link href="http://leetschau.github.io/blog/2014/10/24/150059/"/>
    <updated>2014-10-24T15:00:59+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/10/24/150059</id>
    <content type="html"><![CDATA[<p>WFP算法从告警报文中挖掘出关联规则后，保存在下面的RDD data中，每一条规则包含4项：支持度、置信度、规则前项和规则后项。
前项和后项分别是一个字符串，是一个逗号分隔的多个网元列表，
例如一条规则(0.2, 0.3, &ldquo;a,b&rdquo;, &ldquo;c&rdquo;)表示“网元a,b上的告警导致网元c上告警发生的支持度是0.2，置信度是0.3。
也就是a,b,c在所有告警中发生的概率是20%，当a,b已经发生时，c发生的比率是30%。</p>

<p>在另一张资源表中，每一行包含一对网元，保存在下面的RDD res_data中，
例如"a,c"这一行表示网元a和c有资源上的关联关系，可能是物理链路连接，可能是同一物理位置等等。</p>

<p>所谓的资源过滤就是只有在资源表中的规则才算有效规则，
资源关系是没有顺序的，不论规则(a => c)还是(c => a)都符合(a,c)这一资源约束，
对于规则(x1, x2, &hellip;, xm => y1, y2, &hellip;, yn)，只有前项和后项的所有笛卡尔积</p>

<pre><code>(x1, y1), (x1, y2), ... (x1, yn)
...
(xm, y1), (xm, y2), ... (xm, yn)
</code></pre>

<p>都在资源表中，才表明这一条规则通过了资源过滤。
例如对于规则(a,b => c)，只有a与c，a与b都有资源关系（即(a,b)和(a,c)都在资源表中），这条规则才有效。</p>

<p>实现算法是：对于一条规则R1，求出其所有笛卡尔积R2，然后求R2与资源表的交集R3，如果</p>

<p>下面是描述这一筛选过程的Spark代码:</p>

<pre><code>val data = sc.parallelize(List((0.2, 0.3, "a,b", "c"), (0.5, 0.2, "b,c", "a,d")))
val combine_pre_suf = data.flatMap(x =&gt; (x._3.split(",").flatMap(y =&gt; (x._4.split(",").map(z =&gt; (y+","+z, x))))))
val suf_pre = combine_pre_suf.map(x =&gt; (x._1.split(",")(1) + "," + x._1.split(",")(0), x._2))
val double_pre_suf_rule = suf_pre ++ combine_pre_suf
val res_data = sc.parallelize(List("a,c","b,c","c,e","c,d"))
val res_join_double = res_data.map(x =&gt; (x,1)).join(double_pre_suf_rule)
val rule_in_res_cnt = res_join_double.map(x =&gt; (x._2._2, x._2._1)).reduceByKey(_+_)
val flt_res_rules = rule_in_res_cnt.filter(x =&gt; x._1._3.split(",").size * x._1._4.split(",").size == x._2)
</code></pre>

<p>将这段代码保存在文件res-filter.script中，运行<code>spark-shell -i res-filter.script</code>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Spark Tasks on ArchLinux]]></title>
    <link href="http://leetschau.github.io/blog/2014/09/15/110702/"/>
    <updated>2014-09-15T11:07:02+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/09/15/110702</id>
    <content type="html"><![CDATA[<h1>Install</h1>

<ol>
<li><p>Install JDK: <code>sudo pacman -S jdk7-openjdk</code>;</p></li>
<li><p>Download pre-build package (spark-1.1.0-bin-hadoop2.4.tgz) from <a href="http://spark.apache.org/">Apache Spark website</a>;</p></li>
<li><p>Extract and add path: <code>tar xf spark-1.1.0-bin-hadoop2.4.tgz</code>;</p></li>
</ol>


<h1>Run Spark script</h1>
]]></content>
  </entry>
  
</feed>
