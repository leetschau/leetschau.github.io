<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Utf-8 | Dark Matter in Cyberspace]]></title>
  <link href="http://leetschau.github.io/blog/categories/utf-8/atom.xml" rel="self"/>
  <link href="http://leetschau.github.io/"/>
  <updated>2015-08-11T14:15:46+08:00</updated>
  <id>http://leetschau.github.io/</id>
  <author>
    <name><![CDATA[Li Chao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Unicode到UTF-8编码转换的Java实现]]></title>
    <link href="http://leetschau.github.io/blog/2012/12/17/151751/"/>
    <updated>2012-12-17T15:17:51+08:00</updated>
    <id>http://leetschau.github.io/blog/2012/12/17/151751</id>
    <content type="html"><![CDATA[<p>Unicode到UTF-8转换的规则见笔记 字符编解码的故事 ，下面将转换过程代码化，以演示如何手工对byte数据进行操作，以及需要注意的问题（字节位的高低定义是：high<-->low）。</p>

<p> package encoding;
 import java.io.UnsupportedEncodingException;
 import org.junit.Test;
 public class Converter {
  public static void main(String[] args) throws UnsupportedEncodingException {
   char chnChar = &lsquo;联&rsquo;;
   System.out.println(&ldquo;UTF-8 bytes of &rdquo; + chnChar + &ldquo;: &rdquo; + convUnicode2utf8(chnChar));
  }
  /<em>*
   * 演示unicode到utf-8的转换过程。
   * @param 要进行转换的汉字
   * @throws UnsupportedEncodingException
   * @return 16进制表示的汉字UTF-8编码字节序列
   </em>/
  public static String convUnicode2utf8(char input) throws UnsupportedEncodingException {
   int lowByte = input &amp; 0xff;
   int highByte = (input &amp; 0xff00) >>> 8;    // 第二次运行时注释掉本行
   // byte highByte = (byte) ((input &amp; 0xff00) >>> 8); // 第二次运行时取消注释本行
   System.out.println(&ldquo;Unicode bytes of &rdquo; + input + &ldquo;: &rdquo; + Integer.toHexString(highByte)
     + &ldquo;, &rdquo; + Integer.toHexString(lowByte));
   // UTF-8的第1个字节是1110 + highByte前4位
   int high4inHighByte = highByte >>> 4;
   System.out.println(&ldquo;highByte>>>4: hex:&rdquo; + Integer.toHexString(high4inHighByte)
     + &ldquo;, demical:&rdquo; + high4inHighByte);
   int utf8Byte1 = (7 &lt;&lt; 5) + high4inHighByte;
   // UTF-8的第2个字节是10 + highByte后4位 + lowByte前2位
   int low4inHighByte = highByte &amp; 0xf;
   int high2inLowByte = lowByte >>> 6;
   int utf8Byte2 = (1 &lt;&lt; 7) + (low4inHighByte &lt;&lt; 2) + high2inLowByte;
   // UTF-8的第3个字节是10 + lowByte后6位
   int utf8Byte3 = (1 &lt;&lt; 7) + (lowByte &amp; 0x3f);
   String result = Integer.toHexString(utf8Byte1) + &ldquo;, &rdquo; + Integer.toHexString(utf8Byte2)
     + &ldquo;, &rdquo; + Integer.toHexString(utf8Byte3);
   return result;
  }
   public static String bytes2HexString(byte[] b) {
   String ret = &ldquo;&rdquo;;
   for (int i = 0; i &lt; b.length; i++) {
    String hex = Integer.toHexString(b[i] &amp; 0xFF);
    if (hex.length() == 1) {
     hex = &lsquo;0&rsquo; + hex;
    }
    ret += hex;
   }
   return ret;
  }
  }</p>

<p>第1次运行结果：</p>

<p> Unicode bytes of 联: 80, 54
 highByte>>>4: hex:8, demical:8
 UTF-8 bytes of 联: e8, 81, 94</p>

<p>第2次运行结果：</p>

<p> Unicode bytes of 联: ffffff80, 54
 highByte>>>4: hex:ffffff8, demical:268435448
 UTF-8 bytes of 联: 100000d8, 81, 94</p>

<h1>结果分析</h1>

<p>当highByte为byte型时（第二次运行），"Integer.toHexString(highByte)&ldquo;的运行结果是ffffff80，这是由于toHexString(int i)方法会先将i转换为int型，Java中没有无符号数的概念，0x80作为byte型数据的值是-128，转换成int型的-128就是ffffff80。后面的highByte>>>4也一样，移位操作符（>>和>>>）要求左右的操作数是int或者long，highByte首先被转换为int值0xffffff80，然后无符号右移4位，变为0x0ffffff8，即268435448，最后导致utf8Byte1得到错误的值。可见错误的根本原因在于移位运算符对byte型数据按有符号数进行了“私下”转换。</p>

<h1>解决方法</h1>

<p>当要对数据进行字节位操作时，要特别注意数据是有符号还是无符号的，如果是无符号的（例如汉字编码），并且要进行移位操作（主要是向右移位，向左移位不受是否有符号位影响），应将被处理的byte保存在int型变量中。</p>

<p>参考<a href="http://stackoverflow.com/questions/3948220/behaviour-of-unsigned-right-shift-applied-to-byte-variable%EF%BC%8C%E7%A7%BB%E4%BD%8D%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E8%AF%B4%E6%98%8E%E5%8F%82%E8%80%83">http://stackoverflow.com/questions/3948220/behaviour-of-unsigned-right-shift-applied-to-byte-variable%EF%BC%8C%E7%A7%BB%E4%BD%8D%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E8%AF%B4%E6%98%8E%E5%8F%82%E8%80%83</a>"Java in a Nutsehll"一书中"Bitwise and Shift Operators"一节。</p>

<h1>网络传输中的汉字和特殊字符</h1>

<p>Java程序用字节数组接收网络传输数据时，字节的值只要超过0x7f，直接打印出来就是负数，这是Java对byte类型的定义（-128~127）造成的，不是错误，需要打印日志或者屏显时用上面代码中的bytes2HexString()打印hex码。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux平台上转换文件编码]]></title>
    <link href="http://leetschau.github.io/blog/2012/12/13/154337/"/>
    <updated>2012-12-13T15:43:37+08:00</updated>
    <id>http://leetschau.github.io/blog/2012/12/13/154337</id>
    <content type="html"><![CDATA[<p>Linux系统的iconv指令是一个很好的文件编码转换工具，支持的编码范围广，使用方便，例如将一个utf-8编码的文件（名为tic）转换为gbk编码：</p>

<p>iconv -f utf-8 -t gbk tic > ticgbk</p>

<p>可以用"iconv -l"查看系统支持的所有编码列表。</p>

<p>另：用file -i <file_name>可以查看一个文件的编码方式。</p>

<p>下面是一个可以一次转换多个文件的脚本：</p>

<pre><code>mkdir dst
for file in $(find src -type f); do
  iconv -f gbk -t utf-8 $file &gt; dst/$(basename $file)
done
</code></pre>

<p>这里要注意的是，由于重定向的目标文件所在的目录必须已经存在，所以需要首先创建目标目录dst，其次find命令输出的文件列表是以当前目录为基准，所以输出目标必须去掉路径部分，也就是"basename $file"做的，且源目录src中不能有子目录，否则所有目录下的文件会被摊平放在dst下，如果不同子目录下有同名文件，则最后一个会覆盖前面同名文件的内容。</p>

<p>如果确实需要转换包括子目录的所有文件，可以通过一个临时目录中转，完成该过程的脚本"myconv.sh"内容如下：</p>

<pre><code>#!/bin/bash
if [[ $# != 2 ]]; then
    echo Usage: ./myconv.sh src dst
    exit 1
fi
src=$1
dst=$2
mkdir -p tmp
cp -r $src tmp/
for file in $(find $src -type f); do
    #echo iconv from $file to $dst/$file
    iconv -f gbk -t utf-8 $file &gt; tmp/$file
done
mv tmp/$src $dst
rm -rf tmp
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux上中文乱码问题的解决办法]]></title>
    <link href="http://leetschau.github.io/blog/2012/06/19/163003/"/>
    <updated>2012-06-19T16:30:03+08:00</updated>
    <id>http://leetschau.github.io/blog/2012/06/19/163003</id>
    <content type="html"><![CDATA[<p>中文出现乱码，首先需要查看字符串编码和解码的方法是否一致，其次检查Linux的LANG环境变量中的编码是否匹配程序的编解码方法。</p>

<p>建议统一编码为UTF-8，对应的bash脚本：export LANG=zh_CN.UTF-8</p>

<p>编码是字符串转换为字节数组（二进制串），解码是逆过程，charset指定了其中转换/逆转换的方式（见 字符编解码的故事 ）。就Java而言，用指定的charset（例如UTF-8）对字符串编码用"byte[] dst = input_string.getBytes(&ldquo;UTF-8&rdquo;);&ldquo;实现，解码用“String dst = new String(input_byte_array, "UTF-8&rdquo;);”实现；</p>

<p>如果编码使用“byte[] dst = input_string.getBytes()”，则具体的编码方式依赖于操作系统，存在编码错误的风险，解码用“String dst = new String(input_byte_array);”也存在解码错误的风险，所以在编/解码过程中最好明确指定使用的charset。</p>

<p>下面的代码将字节数组转换为16进制字节码字符串，以便观察不同的charset如何影响生成的字节码。</p>

<p> public class MQReceiver {
 public static void main(String[] args) {
  byte[] buf = mq.recvTextMessage(); // receive msg from mq
  if (buf != null) {
   String hexRes = bytes2HexString(buf);
   System.out.println(hexRes);
   System.out.println(getChnBytes(hexRes));
   String bufstr = new String(buf,&ldquo;UTF-8&rdquo;);
   System.out.println(&ldquo;Recv alarm: &rdquo; + bufstr);
  } else {
   System.out.println(&ldquo;buf: &rdquo; + buf);
  }
 }</p>

<p> public static String bytes2HexString(byte[] b) {
  String ret = &ldquo;&rdquo;;
  for (int i = 0; i &lt; b.length; i++) {
   String hex = Integer.toHexString(b[i] &amp; 0xFF);
   if (hex.length() == 1) {
    hex = &lsquo;0&rsquo; + hex;
   }
   ret += hex.toUpperCase();
  }
  return ret;
 }
 public static String getChnBytes(String input) {
  String sflag = &ldquo; &lt;FM_ALARM_MSG.AlarmText> &rdquo;;
  String eflag = &ldquo;&lt;/FM_ALARM_MSG.AlarmText>&rdquo;;
  byte[] start = sflag.getBytes();
  byte[] end = eflag.getBytes();
  String ss = bytes2HexString(start);
  String se = bytes2HexString(end);
  int beginIndex = input.indexOf(ss) + ss.length();
  int endIndex = input.indexOf(se);
  return input.substring(beginIndex, endIndex);
 }
}</p>

<p>其中getChnBytes方法从下面的字符数组中找到汉字，提取出来，以便于观察汉字部分编码的变化，</p>

<p> &lt;FM_ALARM_MSG.EventTime>2012-06-20 14:39:51&lt;/FM_ALARM_MSG.EventTime> &lt;FM_ALARM_MSG.AlarmText> 检查数据库连接产生异常&lt;/FM_ALARM_MSG.AlarmText><C_FP0></C_FP0></p>

<p>例如UTF-8的“ 检查数据库连接产生异常”编码输出如下：</p>

<p> E6A380
E69FA5
E695B0
E68DAE
E5BA93
E8BF9E
E68EA5
E4BAA7
E7949F
E5BC82
E5B8B8</p>

<p>注： 快典网 可以在线查看汉字的各种编码。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文字的编码问题]]></title>
    <link href="http://leetschau.github.io/blog/2010/10/12/135950/"/>
    <updated>2010-10-12T13:59:50+08:00</updated>
    <id>http://leetschau.github.io/blog/2010/10/12/135950</id>
    <content type="html"><![CDATA[<p>我们需要将文字保存在磁盘上，但磁盘上只能存储0和1（实际上是存储介质的两种状态），不能存储文字，这就出现了一个问题，如何将文字转换为二进制数字串？</p>

<p>文件的编码/解码就是解决文字&lt;=>二进制串这一环节如何相互转换的问题。</p>

<p> 简单地说，GBK和Unicode分别是一种码表，也就是为每一个字符指定一个两个字节组成的代码，例如 “汉”字的Unicode编码为0x6C49，GBK编码为0xBABA（0x是一个字头，表示后面的是16进制字串）。</p>

<p> Unicode编码 Unicode由ISO（国际标谁化组织）制定，它虽然解决了各种语言之间转换的难题，但也有问题，首先是不论什么字符都2个字节表示，网络上英文信息占大部分，由此造成的空间浪费很可观，其次是 Unicode不兼容ASCII编码方案，为了解决这些问题，出现了 UTF-8方案。</p>

<p> Unicode到UTF-8的转换方法详见 字符编解码的故事 。</p>

<p> GBK编码 GBK编码规则：《汉字内码扩展规范(GBK)》（ 全国信息技术标准化技术委员会 ）：
 01-09区为特殊符号；16-55区为一级汉字，按拼音排序；56-87区为二级汉字，按部首/笔画排序。
每个汉字及符号以两个字节来表示。第一个字节称为“高位字节”，第二个字节称为“低位字节”，用这个字的区、位号加上0xA0就得到了对应的字节码。例如“啊”字是 第16区第1个字，所以 区位码是 1601 ， 编码方法：0xA0+ 16 = 0x B0 ， 0xA0+ 1 = 0x A1 ，所以得到“啊”的 字节码  B0A1 ，可以用UltraEdit查看字符的字节码验证。</p>

<p> GB2312是GBK的早期版本，能表示6000个汉字，由于容量较小，现已被GBK取代。</p>

<p> 解码过程</p>

<p> 在Windows系统中打开文件时，使用猜的方式选择解码方案。如果文件开头使用了FEFF或FFFE，就认为是Unicode/UTF-8编码，否则为ANSI编码，在中文系统下，就是用GBK解码。GBK解码时，如果一个字节大于 0x7F（ 127），就证明这个字节与后面的字节组成了一个汉字，由于汉字的字节码总大于0xA0，英文字节码总小于 0x7F ，因此二者泾渭分明，不会混淆。</p>

<p> 用猜的方式确定文件的编码方案绝大多数情况下没有问题，但凡事总有特例，详见 字符编解码的故事 对“联通”编码/解码过程的说明。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VIM的文件编码问题]]></title>
    <link href="http://leetschau.github.io/blog/2010/08/05/112702/"/>
    <updated>2010-08-05T11:27:02+08:00</updated>
    <id>http://leetschau.github.io/blog/2010/08/05/112702</id>
    <content type="html"><![CDATA[<h1>转换文件编码</h1>

<p>:set fileencoding=gbk/utf-8</p>

<h1>关于vim的编码</h1>

<p>Vim 有四个跟字符编码方式有关的选项，encoding、fileencoding、fileencodings、termencoding (这些选项可能的取值请参考 Vim 在线帮助 :help encoding-names)，它们的意义如下:
1. encoding：Vim内部（软件自身）使用的字符编码方式，包括 Vim 的 buffer (缓冲区)、菜单文本、消息文本等。默认是根据你的locale选择。用户手册上建议只在 .vimrc 中改变它的值，事实上也只有在.vimrc 中改变它的值才有意义。为了理解这个参数的含义，可做如下实验：
启动一个VIM，用:set encoding查询当前该参数的值，例如为utf-8，然后执行:set encoding=latin1，然后随便写一个命令让VIM提示出错，你会发现提示信息全是乱码。
反过来，如果你的VIM在运行后提示信息、菜单等处是乱码，可以用:set encoding=&hellip;来尝试一下别的编码。你可以用另外一种编码来编辑和保存文件，如你的vim的encoding为utf-8，所编辑的文件采用cp936编码，VIM会自动将读入的文件转成utf-8(VIM的能读懂的方式），而当你写入文件时，又会自动转回成GBK（文件的保存编码)。
1. fileencoding：VIM中当前编辑的文件的字符编码方式，VIM保存文件时也会将文件保存为这种字符编码方式 (不管是否新文件都如此)，可用:set fileencoding查询当前文件的编码类型，用:set fileencoding=utf-8将当前文件转换为utf-8格式。
1. fileencodings：VIM读文件时尝试的编码方案的列表，启动时VIM按照它所列出的字符编码方式逐一探测即将打开的文件的字符编码方式，并且将 fileencoding 设置为最终尝试成功的字符编码方式。因此最好将Unicode 编码方式放到这个列表的最前面，将拉丁语系编码方式 latin1 放到最后面。
1. termencoding：VIM所工作的终端 (或者 Windows 的 Console 窗口) 的字符编码方式。如果VIM所在的term与VIM编码相同，则无需设置。如其不然，你可以用VIM的termencoding选项将自动转换成term 的编码。这个选项对Windows 下的gVim 无效（gVim是GUI 模式的，不是Console模式），因此无需关心这个参数，一般置空就行了。
Windows下的gVim的vimrc文件关于文件编码部分应设为：
set encoding=utf-8
set fileencodings=utf-8,chinese,latin-1
set fileencoding=utf-8
其中编码方案的第二项chinese主要是为了保持与windows上的很多默认编码格式兼容（如记事本的ANSI格式）。chinese 是个别名，在Unix 里表示gb2312，在Windows里表示cp936，也就是GBK与微软的ANSI兼容。当VIM打开cp936格式的文件时，编辑完后不要用:set fileencoding=utf-8命令将其转换为utf-8格式再保存，因为VIM生成的utf-8格式文件不带BOM表，与windows仍不兼容。</p>
]]></content>
  </entry>
  
</feed>
