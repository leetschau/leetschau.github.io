<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloudera | Dark Matter in Cyberspace]]></title>
  <link href="http://leetschau.github.io/blog/categories/cloudera/atom.xml" rel="self"/>
  <link href="http://leetschau.github.io/"/>
  <updated>2015-02-02T16:10:00+08:00</updated>
  <id>http://leetschau.github.io/</id>
  <author>
    <name><![CDATA[Li Chao]]></name>
    <email><![CDATA[leetschau@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Run MapReduce Jobs on Spark]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/15/175039/"/>
    <updated>2014-08-15T17:50:39+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/15/175039</id>
    <content type="html"><![CDATA[<h1>Interactive Mode</h1>

<p>Follow <a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start of Spark 1.0.2 on Apache Spark website</a>.</p>

<pre><code># hadoop fs -put alarm_data_for_explore-0501-0505.txt alarm_data_for_explore-0501-0505.txt 

# hadoop fs -ls
...
-rw-r--r--   3 root root  260780698 2014-08-15 16:45 alarm_data_for_explore-0501-0505.txt

# wc -l alarm_data_for_explore-0501-0505.txt
1362005

# head -1 alarm_data_for_explore-0501-0505.txt 
-2117657102|102|1000012276|License...|3|4|102|

# grep 007-002-00-000592 alarm_data_for_explore-0501-0505.txt | wc -l
12

# mv alarm_data_for_explore-0501-0505.txt aaa.txt       // this local file is unnecessary any more

# spark-shell

scala&gt; val textFile = sc.textFile("alarm_data_for_explore-0501-0505.txt")
textFile: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at &lt;console&gt;:12

scala&gt; textFile.count()
...
res5: Long = 1362005

scala&gt; textFile.first()
...
res8: String = -2117657102|102|1000012276|License...|3|4|102|

scala&gt; textFile.filter(line =&gt; line.contains("007-002-00-000592")).count()
...
res12: Long = 12

scala&gt; import java.lang.Math
import java.lang.Math

scala&gt; textFile.map(line =&gt; line.split("|").size).reduce((a, b) =&gt; Math.max(a, b))
...
res13: Int = 372
</code></pre>

<h2>Run Spark Script</h2>

<pre><code>$ cat wfp-spark
val MIN_SUP = 0.0003
val MIN_CONF = 0
val MAX_RELATION_ORDER = 3
val DATA_FILE = "input"

val textFile = sc.textFile(DATA_FILE)
val weight = textFile.map(x =&gt; x -&gt; x.split(",")(5).toFloat).cache
val data = textFile.groupBy(x =&gt; x.split(",")(0))

$ spark-shell -i wfp-spark
...
</code></pre>

<p>Now you are in a spark shell, all variables such as MIN_SUP, MIN_CONF are accessible.</p>

<h1>Batch Mode</h1>

<p>Follow <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_running_spark_apps.html">Running Spark Applications in CDH 5 Installation Guide</a>.</p>

<pre><code># locate spark-defaults.conf       // find out where is $SPARK_HOME
/etc/spark/conf.cloudera.spark/spark-defaults.conf
...

# ll /etc/spark/conf.cloudera.spark
...
-rw-r--r-- 1 root root 883 Aug 14 10:12 spark-env.sh

# cat /etc/spark/conf.cloudera.spark/spark-env.sh
...
export SPARK_HOME=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/spark
...

# source /etc/spark/conf.cloudera.spark/spark-env.sh     // load $SPARK_HOME, etc

# spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode client --master yarn $SPARK_HOME/examples/lib/spark-examples_2.10-1.0.0-cdh5.1.0.jar 10
</code></pre>
]]></content>
  </entry>
  
</feed>
