<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Xpath | Dark Matter in Cyberspace]]></title>
  <link href="http://leetschau.github.io/blog/categories/xpath/atom.xml" rel="self"/>
  <link href="http://leetschau.github.io/"/>
  <updated>2015-08-11T14:15:46+08:00</updated>
  <id>http://leetschau.github.io/</id>
  <author>
    <name><![CDATA[Li Chao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scrapy Notes]]></title>
    <link href="http://leetschau.github.io/blog/2015/03/30/175224/"/>
    <updated>2015-03-30T17:52:24+08:00</updated>
    <id>http://leetschau.github.io/blog/2015/03/30/175224</id>
    <content type="html"><![CDATA[<p>Install on Ubuntu/Mint:</p>

<pre><code>sudo apt-get install python-twisted python-libxml2 python-simplejson
sudo pip install scrapy
</code></pre>

<p>Create an alias in ~/.bash_aliases for convenience: <code>alias sa='scrapy'</code>.</p>

<p>Crawl website in shell:</p>

<pre><code>$ scrapy shell http://myexpo.com/exhibition/32191.html
...
[1] name = response.xpath('//h1/text()').extract()[0]
[2] time = response.xpath('//div[@class="location"]/span[1]/text()').extract()[0].split('\r\n')[1].strip()
[3] addr = response.xpath('//div[@class="location"]/span[2]/text()').extract()[0].split('\r\n')[1].strip()
</code></pre>

<h1>Crawl my blog</h1>

<p>$ sa startproject myblog</p>

<p>Ref:</p>

<p><a href="http://blog.csdn.net/u012150179/article/details/32911511">scrapy研究探索（二）——爬w3school.com.cn</a></p>

<h1>Unicode character in Python 2.x</h1>

<p>You have to add u'&lsquo; prefix to a unicode string in Python 2.x.
Their raw value is in &rsquo;\u' format.
To see them in human friendly format, use <code>print()</code> function.</p>

<pre><code>In [85]: ex = u'中国2015年3月'

In [86]: ex
Out[86]: u'\u4e2d\u56fd2015\u5e743\u6708'

In [87]: print(ex)
中国2015年3月

In [88]: ex.find(u'年')
Out[88]: 6
</code></pre>

<p>See more detailed contents about this topic in note &ldquo;Unicode and File I/O in Python 2.X and 3.X&rdquo;.</p>

<h1>XPath Grammar</h1>

<p>Ref: <a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp">XPath 语法</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用lxml模块解析JUnit生成的测试报告]]></title>
    <link href="http://leetschau.github.io/blog/2011/01/11/100823/"/>
    <updated>2011-01-11T10:08:23+08:00</updated>
    <id>http://leetschau.github.io/blog/2011/01/11/100823</id>
    <content type="html"><![CDATA[<p>JUnit的测试结果首先保存在xml文档中，然后生成html格式的测试报告，这份报告中的数据是用一段javascript代码根据 xml文档的数据 计算出来的，因此我们要获取测试结果数据，就只能分析html文档，这方面lxml模块（codespeak.net/lxml/）的功能比较令人满意，利用lxml的xpath语法可以方便地得到某个标签的内容，下面是一个分析样例文本：</p>

<p> &lt;!DOCTYPE HTML PUBLIC &ldquo;-//W3C//DTD HTML 4.01 Transitional//EN&rdquo;></p>

<p> <html xmlns:lxslt="http://xml.apache.org/xslt" xmlns:stringutils="xalan://org.apache.tools.ant.util.StringUtils"></p>

<p> <head></p>

<p> <META http-equiv="Content-Type" content="text/html; charset=US-ASCII"></p>

<p> <title>Unit Test Results.</title></p>

<p> <script type="text/javascript" language="JavaScript"></p>

<p> &hellip;(1000多行)</p>

<p> </script></p>

<p> </head></p>

<p> <body></p>

<p> <a name="top"></a></p>

<p> <h1>Unit Test Results.</h1></p>

<p> <table width="100%"></p>

<p> <tr></p>

<p> <td align="left"></td><td ....</td></p>

<p> </tr></p>

<p> </table></p>

<p> <hr size="1"></p>

<p> <h2>Summary</h2></p>

<p> <table class="details" border="0" cellpadding="5" cellspacing="2" width="95%"></p>

<p> <tr valign="top"></p>

<p> <th>Tests</th><th>Failures</th><th>Errors</th><th>Success rate</th><th>Time</th></p>

<p> </tr></p>

<p> <tr valign="top" class="Error"></p>

<p> <td>24</td><td>0</td><td>24</td><td>0.00%</td><td>0.155</td></p>

<p> </tr></p>

<p> </table></p>

<p> <table ...></p>

<p> <tr ...></p>

<p> </tr></p>

<p> </table></p>

<p> &hellip;</p>

<p> </body></p>

<p> </html></p>

<p>效果见下图：</p>

<p>解析html的目的是取出文本中黑体字表示的数据。</p>

<p>解析函数是：</p>

<p> from lxml import html</p>

<p> def parseJUnitReport(html_filename):</p>

<p> html_doc = html.parse(html_filename).getroot()</p>

<p> res_tbl = html_doc.xpath(&ldquo;//html//body//table&rdquo;)[1]</p>

<p> test_item_names = [k.text for k in res_tbl.xpath(&lsquo;.//tr//th&rsquo;)] # list comprehension</p>

<p> test_datasets = [k.text for k in res_tbl.xpath(&lsquo;.//tr//td&rsquo;)]</p>

<p> return dict(zip(test_item_names,test_datasets))</p>

<p>测试代码：</p>

<p> aa = &lsquo;e:\BVT\GODU-BVT\GCIF\build\GAPI\build\result\junit\junit-noframes.html&rsquo;</p>

<p> print build.parseJUnitReport(aa)</p>

<p>运行结果如下：</p>

<p> {&lsquo;Failures&rsquo;: &lsquo;0&rsquo;, &lsquo;Tests&rsquo;: &lsquo;24&rsquo;, &lsquo;Errors&rsquo;: &lsquo;24&rsquo;, &lsquo;Success rate&rsquo;: &lsquo;0.00%&rsquo;, &lsquo;Time&rsquo;: &lsquo;0.155&rsquo;}</p>

<p>从解析函数可以看到：</p>

<p>导入lxml.html后，其parse方法的参数是一个文件路径字符串，用xpath()方法得到的是一个包含<body>内所有的<table>的list，由于目标table位于第二位，所以取[1]得到table对象res_tbl，然后在它的下面取<tr>里的<th>，得到标题行，对于每个得到的节点，需要用它的text属性得到文本内容，这里使用了list的comprehension功能，用一行代码就取到每个元素的text属性并组成了新的list（test_item_names和test_datasets），然后用zip方法将两个list组合成了一个list，再用dict方法将list转换成了字典。</p>
]]></content>
  </entry>
  
</feed>
