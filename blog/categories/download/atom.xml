<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Download | Dark Matter in Cyberspace]]></title>
  <link href="http://leetschau.github.io/blog/categories/download/atom.xml" rel="self"/>
  <link href="http://leetschau.github.io/"/>
  <updated>2016-11-11T14:51:43+08:00</updated>
  <id>http://leetschau.github.io/</id>
  <author>
    <name><![CDATA[Li Chao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[加速pip下载速度]]></title>
    <link href="http://leetschau.github.io/blog/2016/10/07/223030/"/>
    <updated>2016-10-07T22:30:30+08:00</updated>
    <id>http://leetschau.github.io/blog/2016/10/07/223030</id>
    <content type="html"><![CDATA[<p>有时候使用<code>pip</code>安装第三方库时，下载速度很慢，导致经常因为超时安装失败。
解决方法是修改<code>pip</code>安装源，指向一个本地的安装源，下面以安装csvkit库为例说明。</p>

<ol>
<li><p>打开<a href="https://pypi-mirrors.org/">PyPI Mirror Status</a>，选择一个本地（北京）的安装源；</p></li>
<li><p>打开对应的网址，例如<a href="https://pypi.douban.com/">豆瓣的pip镜像</a>，下面会有一个
名为<code>simple</code>的文件夹，看下面有没有<code>csvkit</code>目录，如果有，说明这个源可用，
简单的方法是在源的URL后直接加<code>simple/csvkit</code>：<code>https://pypi.doubanio.com/simple/csvkit</code>
看这个地址是否存在；</p></li>
<li><p>确认源存在后，加<code>-i</code>参数安装：<code>pip install --user -i https://pypi.doubanio.com/simple/ csvkit</code>;</p></li>
</ol>


<p>说明：</p>

<p>用<code>pip install --help</code>查看参数使用说明。网上有说加<code>--use-mirrors</code>参数自动选择镜像，
但Ubuntu 16.04上安装的<code>pip</code>并没有<code>--use-mirrors</code>这个参数（用<code>pip -V</code>查询：
pip 8.1.2 on Python 2.7）。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speed Up Downloading Files on Linux]]></title>
    <link href="http://leetschau.github.io/blog/2014/02/21/081656/"/>
    <updated>2014-02-21T08:16:56+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/02/21/081656</id>
    <content type="html"><![CDATA[<p>Compared aria2c, axel and wget, aria2c is the best. It support multi-thread download (with &ldquo;-s <num>&rdquo;) and resume partially downloaded files automatically (you have to use &ldquo;-c&rdquo; in wget for this function). Synopsis:</p>

<pre><code>$ aria2c -s 10 &lt;URL&gt;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Download Website With Wget]]></title>
    <link href="http://leetschau.github.io/blog/2013/12/13/173402/"/>
    <updated>2013-12-13T17:34:02+08:00</updated>
    <id>http://leetschau.github.io/blog/2013/12/13/173402</id>
    <content type="html"><![CDATA[<p>I want to download the Chinese version of &ldquo;Learn you a Haskell for Great Good&rdquo; at <a href="http://learnyouahaskell-zh-tw.csie.org/,">http://learnyouahaskell-zh-tw.csie.org/,</a> use this command:</p>

<pre><code>wget -U mozilla --limit-rate=200k --no-clobber --convert-links --random-wait -r -p -E -e robots=off http://learnyouahaskell-zh-tw.csie.org/
</code></pre>

<p>where:</p>

<p> -U mozilla: pretends to be just like a browser Mozilla is looking at a page;</p>

<p> -r: recursive download all website, -p: download everything even pictures;</p>

<p> -E: gets the right extension of the file, without most html and other files have no extension;</p>

<p> -e robots=off: act like we are not a robot;</p>

<p>You can use &ldquo;-l&rdquo; option to specify the recursive depth. The default maximum depth is five layers.</p>

<p>Ref:</p>

<p><a href="http://www.kossboss.com/linux---wget-full-website">LINUX - wget full website - full site download</a></p>

<p><a href="https://www.gnu.org/software/wget/manual/html_node/Recursive-Download.html">https://www.gnu.org/software/wget/manual/html_node/Recursive-Download.html</a></p>
]]></content>
  </entry>
  
</feed>
