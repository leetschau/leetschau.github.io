<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scala | Dark Matter in Cyberspace]]></title>
  <link href="http://leetschau.github.io/blog/categories/scala/atom.xml" rel="self"/>
  <link href="http://leetschau.github.io/"/>
  <updated>2015-02-02T16:10:00+08:00</updated>
  <id>http://leetschau.github.io/</id>
  <author>
    <name><![CDATA[Li Chao]]></name>
    <email><![CDATA[leetschau@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scala的模式匹配]]></title>
    <link href="http://leetschau.github.io/blog/2014/12/16/170917/"/>
    <updated>2014-12-16T17:09:17+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/12/16/170917</id>
    <content type="html"><![CDATA[<p>下面是一个RDD，每一个元素是一个Array，这个Array由几个tuple组成，
现在需要将其中第二个元素是13的tuple（比如(-36522769,13)）的一个元素后面追加字符串&#8221;.000&#8221;，其余不变，
实现方法如下：</p>

<pre><code>val data = RDD[Array[(String, Int)]] = RDD(
        Array((-36522769,13), (8014,14), (2014-09-19 00:10:47,17), (3,18), (FF-693960651,134)),
        Array((-905910021.0000000000,13), (200,14), (2014-09-19 00:02:11,17), (3,18), (FF--112226702,134)),
        Array((-2033866158.0000000000,13), (200,14), (2014-09-19 00:07:12,17), (3,18), (FF--112226702,134)),
        Array((1712341836.0000000000,13), (200,14), (2014-09-19 00:07:06,17), (3,18), (FF--112226702,134)),
        Array((-896432680.0000000000,13), (8014,14), (2014-09-19 00:12:17,17), (3,18), (FF-693960651,134)),
        Array((-1205034451.0000000000,13), (200,14), (2014-09-19 00:07:21,17), (3,18), (FF--112226702,134)),
        Array((-2117505875.0000000000,13), (15089,14), (2014-09-19 00:41:47,17), (2,18), (FF-1139491304,134))) 

val res = data.map(x =&gt; x.map { case (y,13) =&gt; (y + ".000", 13)
                                case z =&gt; z
                              })
</code></pre>

<p>如果把13变为一个变量，比如<code>val BAD_NUM = 13</code>, 这时直接改写为<code>case (y, BAD_NUM) =&gt; ...</code>，将出现错误，
因为解析器将BAD_NUM视为一个自由变量，而不是外部等于13的那个变量，解决方法是用反引号把BAD_NUM扩起来，写为：</p>

<pre><code>case (y, `BAD_NUM`) =&gt; (y + ".000", BAD_NUM)
</code></pre>

<p>推荐文章：</p>

<p><a href="http://kerflyn.wordpress.com/2011/02/14/playing-with-scalas-pattern-matching/">Playing with Scala’s pattern matching</a>;</p>

<p><a href="http://docs.scala-lang.org/tutorials/tour/pattern-matching.html">Pattern Matching in Scala Documenation</a></p>

<p><a href="http://docs.scala-lang.org/cheatsheets/">Pattern matching section in Scala cheatsheet</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[告警资源过滤算法]]></title>
    <link href="http://leetschau.github.io/blog/2014/10/24/150059/"/>
    <updated>2014-10-24T15:00:59+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/10/24/150059</id>
    <content type="html"><![CDATA[<p>WFP算法从告警报文中挖掘出关联规则后，保存在下面的RDD data中，每一条规则包含4项：支持度、置信度、规则前项和规则后项。
前项和后项分别是一个字符串，是一个逗号分隔的多个网元列表，
例如一条规则(0.2, 0.3, &ldquo;a,b&rdquo;, &ldquo;c&rdquo;)表示“网元a,b上的告警导致网元c上告警发生的支持度是0.2，置信度是0.3。
也就是a,b,c在所有告警中发生的概率是20%，当a,b已经发生时，c发生的比率是30%。</p>

<p>在另一张资源表中，每一行包含一对网元，保存在下面的RDD res_data中，
例如&#8221;a,c&#8221;这一行表示网元a和c有资源上的关联关系，可能是物理链路连接，可能是同一物理位置等等。</p>

<p>所谓的资源过滤就是只有在资源表中的规则才算有效规则，
资源关系是没有顺序的，不论规则(a => c)还是(c => a)都符合(a,c)这一资源约束，
对于规则(x1, x2, &hellip;, xm => y1, y2, &hellip;, yn)，只有前项和后项的所有笛卡尔积</p>

<pre><code>(x1, y1), (x1, y2), ... (x1, yn)
...
(xm, y1), (xm, y2), ... (xm, yn)
</code></pre>

<p>都在资源表中，才表明这一条规则通过了资源过滤。
例如对于规则(a,b => c)，只有a与c，a与b都有资源关系（即(a,b)和(a,c)都在资源表中），这条规则才有效。</p>

<p>实现算法是：对于一条规则R1，求出其所有笛卡尔积R2，然后求R2与资源表的交集R3，如果</p>

<p>下面是描述这一筛选过程的Spark代码:</p>

<pre><code>val data = sc.parallelize(List((0.2, 0.3, "a,b", "c"), (0.5, 0.2, "b,c", "a,d")))
val combine_pre_suf = data.flatMap(x =&gt; (x._3.split(",").flatMap(y =&gt; (x._4.split(",").map(z =&gt; (y+","+z, x))))))
val suf_pre = combine_pre_suf.map(x =&gt; (x._1.split(",")(1) + "," + x._1.split(",")(0), x._2))
val double_pre_suf_rule = suf_pre ++ combine_pre_suf
val res_data = sc.parallelize(List("a,c","b,c","c,e","c,d"))
val res_join_double = res_data.map(x =&gt; (x,1)).join(double_pre_suf_rule)
val rule_in_res_cnt = res_join_double.map(x =&gt; (x._2._2, x._2._1)).reduceByKey(_+_)
val flt_res_rules = rule_in_res_cnt.filter(x =&gt; x._1._3.split(",").size * x._1._4.split(",").size == x._2)
</code></pre>

<p>将这段代码保存在文件res-filter.script中，运行<code>spark-shell -i res-filter.script</code>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Notes]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/27/180124/"/>
    <updated>2014-08-27T18:01:24+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/27/180124</id>
    <content type="html"><![CDATA[<h1>Spark Shell</h1>

<h2>Shutdown info log in spark-shell:</h2>

<pre><code>cd $SPARK_HOME/conf    // here it's ~/apps/spark-1.0.2-bin-hadoop2/conf
cp log4j.properties.template log4j.properties
sed -i '2s/INFO/WARN/g' log4j.properties
</code></pre>

<p>The sed command here replace &ldquo;INFO&rdquo; with &ldquo;WARN&rdquo; in the second line of log4j.properties.</p>

<h2>on cloud61</h2>

<p>Modify /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/spark/conf/log4j.properties, add:</p>

<pre><code>log4j.appender.filelog=org.apache.log4j.RollingFileAppender
log4j.appender.filelog.layout=org.apache.log4j.PatternLayout
log4j.appender.filelog.File=sparkShell.log
log4j.appender.filelog.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
</code></pre>

<h2>Customize shell parameters</h2>

<h3>master and memory</h3>

<p>spark-shell &ndash;master spark://cloud60:7077 &ndash;driver-memory 2g &ndash;executor-memory 2g -i wfp-spark.script</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Closure in Different Languages]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/20/104236/"/>
    <updated>2014-08-20T10:42:36+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/20/104236</id>
    <content type="html"><![CDATA[<p>The following code snippets compare closures in several languages.</p>

<p>First is closure in javascript. Here I use <a href="https://github.com/creationix/nvm">nvm</a> to run javascript code.</p>

<pre><code>$ cat closure.js
function extent() {
    var n = 0;
    return function() {
        n++;
        console.log("n=" + n);
    }
}

extent()();
extent()();
f = extent();
console.log("-----");
//console.log(extent.uniqueId());
f();
f();

$ nvm alias default 0.10
$ nvm run closure.js
Running node v0.10.30
n=1
n=1
-----
n=1
n=2
</code></pre>

<p>In above example, the closure is <code>function() { n++; console.log... }</code>. It&rsquo;s bound to variable &ldquo;f&rdquo;. The local variable &ldquo;n&rdquo; is &ldquo;closed&rdquo; into closure. Its lifetime is the same with &ldquo;f&rdquo;. So after the first execution of f, its inner state is saved.</p>

<p>For Python, nested function can only access variables in outer scope, but can&rsquo;t reassign (modify) them. So I use list as a workaround. In Python 3, there is a new keyword &ldquo;nonlocal&rdquo; to remove this restriction.</p>

<pre><code>$ cat closure.py2.py
def extent():
    n = [0]
    def afun():
        n[0] += 1
        print('n=' + str(n[0]))
    return afun

extent()()
extent()()
print('-----')
f = extent()
f()
f()
print("type of f is %s, its id is %d" % (type(f), id(f)))

$ python closure.py2.py 
n=1
n=1
-----
n=1
n=2
type of f is &lt;type 'function'&gt;, its id is 140403684898408

$ cat closure.py3.py
def extent():
    n = 0
    def afun():
        nonlocal n
        n += 1
        print('n=' + str(n))
    return afun

extent()()
extent()()
f = extent()
print('----')
f()
f()
print("type of f is %s, its id is %d" % (type(f), id(f)))

[0] python3 closure.py3.py
n=1
n=1
----
n=1
n=2
type of f is &lt;class 'function'&gt;, its id is 139914425096528
</code></pre>

<p>The Ruby environment used here is <a href="http://rvm.io/">RVM</a>. Ruby version is 2.1.2.</p>

<pre><code>$ rvm use 2.1.2 --default
$ cat closure.rb
def extent
    n = 0
    lambda {
        n += 1
        printf "n=%d\n", n
    }
end

extent().call()
extent().call()
f = extent()
puts '------'
f.call()
f.call()
puts "class of f is #{f.class.name}, its id is #{f.object_id}"

$ ruby closure.rb
n=1
n=1
------
n=1
n=2
class of f is Proc, its id is 6724520
</code></pre>

<p>Two versions of Scala, define functions via &ldquo;def&rdquo; and &ldquo;val&rdquo; give different results:</p>

<pre><code>$ diff *.scala
1c1
&lt; def extent = {
---
&gt; val extent = {

$ cat closure-def.scala
def extent = {
    var n = 0
    () =&gt; {
        n += 1
        println("n=" + n)
    }
}

extent()
extent()
val f = extent
println("------")
f()
f()
println(s"Class of f is ${f.getClass}, its id is ${f.hashCode}")

$ scala closure-def.scala
n=1
n=1
------
n=1
n=2
Class of f is class Main$$anon$1$$anonfun$extent$1, its id is 8970973

$ cat closure-val.scala 
val extent = {
    var n = 0
    () =&gt; {
        n += 1
        println("n=" + n)
    }
}

extent()
extent()
val f = extent
println("------")
f()
f()
println(s"Class of f is ${f.getClass}, its id is ${f.hashCode}")
$ scala closure-val.scala 
n=1
n=2
------
$ scala closure-val.scala 
n=1
n=2
------
n=3
n=4
Class of f is class Main$$anon$1$$anonfun$1, its id is 8970973
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run MapReduce Jobs on Spark]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/15/175039/"/>
    <updated>2014-08-15T17:50:39+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/15/175039</id>
    <content type="html"><![CDATA[<h1>Interactive Mode</h1>

<p>Follow <a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start of Spark 1.0.2 on Apache Spark website</a>.</p>

<pre><code># hadoop fs -put alarm_data_for_explore-0501-0505.txt alarm_data_for_explore-0501-0505.txt 

# hadoop fs -ls
...
-rw-r--r--   3 root root  260780698 2014-08-15 16:45 alarm_data_for_explore-0501-0505.txt

# wc -l alarm_data_for_explore-0501-0505.txt
1362005

# head -1 alarm_data_for_explore-0501-0505.txt 
-2117657102|102|1000012276|License...|3|4|102|

# grep 007-002-00-000592 alarm_data_for_explore-0501-0505.txt | wc -l
12

# mv alarm_data_for_explore-0501-0505.txt aaa.txt       // this local file is unnecessary any more

# spark-shell

scala&gt; val textFile = sc.textFile("alarm_data_for_explore-0501-0505.txt")
textFile: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at &lt;console&gt;:12

scala&gt; textFile.count()
...
res5: Long = 1362005

scala&gt; textFile.first()
...
res8: String = -2117657102|102|1000012276|License...|3|4|102|

scala&gt; textFile.filter(line =&gt; line.contains("007-002-00-000592")).count()
...
res12: Long = 12

scala&gt; import java.lang.Math
import java.lang.Math

scala&gt; textFile.map(line =&gt; line.split("|").size).reduce((a, b) =&gt; Math.max(a, b))
...
res13: Int = 372
</code></pre>

<h2>Run Spark Script</h2>

<pre><code>$ cat wfp-spark
val MIN_SUP = 0.0003
val MIN_CONF = 0
val MAX_RELATION_ORDER = 3
val DATA_FILE = "input"

val textFile = sc.textFile(DATA_FILE)
val weight = textFile.map(x =&gt; x -&gt; x.split(",")(5).toFloat).cache
val data = textFile.groupBy(x =&gt; x.split(",")(0))

$ spark-shell -i wfp-spark
...
</code></pre>

<p>Now you are in a spark shell, all variables such as MIN_SUP, MIN_CONF are accessible.</p>

<h1>Batch Mode</h1>

<p>Follow <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_running_spark_apps.html">Running Spark Applications in CDH 5 Installation Guide</a>.</p>

<pre><code># locate spark-defaults.conf       // find out where is $SPARK_HOME
/etc/spark/conf.cloudera.spark/spark-defaults.conf
...

# ll /etc/spark/conf.cloudera.spark
...
-rw-r--r-- 1 root root 883 Aug 14 10:12 spark-env.sh

# cat /etc/spark/conf.cloudera.spark/spark-env.sh
...
export SPARK_HOME=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/spark
...

# source /etc/spark/conf.cloudera.spark/spark-env.sh     // load $SPARK_HOME, etc

# spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode client --master yarn $SPARK_HOME/examples/lib/spark-examples_2.10-1.0.0-cdh5.1.0.jar 10
</code></pre>
]]></content>
  </entry>
  
</feed>
