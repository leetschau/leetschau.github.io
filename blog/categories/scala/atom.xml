<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scala | Dark Matter in Cyberspace]]></title>
  <link href="http://leetschau.github.io/blog/categories/scala/atom.xml" rel="self"/>
  <link href="http://leetschau.github.io/"/>
  <updated>2014-09-10T17:28:18+08:00</updated>
  <id>http://leetschau.github.io/</id>
  <author>
    <name><![CDATA[Li Chao]]></name>
    <email><![CDATA[leetschau@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark Notes]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/27/180124/"/>
    <updated>2014-08-27T18:01:24+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/27/180124</id>
    <content type="html"><![CDATA[<ul>
<li>Shutdown info log in spark-shell:</li>
</ul>


<pre><code>cd $SPARK_HOME/conf    // here it's ~/apps/spark-1.0.2-bin-hadoop2/conf
cp log4j.properties.template log4j.properties
sed -i '2s/INFO/WARN/g' log4j.properties
</code></pre>

<p>The sed command here replace &ldquo;INFO&rdquo; with &ldquo;WARN&rdquo; in the second line of log4j.properties.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Closure in Different Languages]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/20/104236/"/>
    <updated>2014-08-20T10:42:36+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/20/104236</id>
    <content type="html"><![CDATA[<p>The following code snippets compare closures in several languages.</p>

<p>First is closure in javascript. Here I use <a href="https://github.com/creationix/nvm">nvm</a> to run javascript code.</p>

<pre><code>$ cat closure.js
function extent() {
    var n = 0;
    return function() {
        n++;
        console.log("n=" + n);
    }
}

extent()();
extent()();
f = extent();
console.log("-----");
//console.log(extent.uniqueId());
f();
f();

$ nvm alias default 0.10
$ nvm run closure.js
Running node v0.10.30
n=1
n=1
-----
n=1
n=2
</code></pre>

<p>In above example, the closure is <code>function() { n++; console.log... }</code>. It&rsquo;s bound to variable &ldquo;f&rdquo;. The local variable &ldquo;n&rdquo; is &ldquo;closed&rdquo; into closure. Its lifetime is the same with &ldquo;f&rdquo;. So after the first execution of f, its inner state is saved.</p>

<p>For Python, nested function can only access variables in outer scope, but can&rsquo;t reassign (modify) them. So I use list as a workaround. In Python 3, there is a new keyword &ldquo;nonlocal&rdquo; to remove this restriction.</p>

<pre><code>$ cat closure.py2.py
def extent():
    n = [0]
    def afun():
        n[0] += 1
        print('n=' + str(n[0]))
    return afun

extent()()
extent()()
print('-----')
f = extent()
f()
f()
print("type of f is %s, its id is %d" % (type(f), id(f)))

$ python closure.py2.py 
n=1
n=1
-----
n=1
n=2
type of f is &lt;type 'function'&gt;, its id is 140403684898408

$ cat closure.py3.py
def extent():
    n = 0
    def afun():
        nonlocal n
        n += 1
        print('n=' + str(n))
    return afun

extent()()
extent()()
f = extent()
print('----')
f()
f()
print("type of f is %s, its id is %d" % (type(f), id(f)))

[0] python3 closure.py3.py
n=1
n=1
----
n=1
n=2
type of f is &lt;class 'function'&gt;, its id is 139914425096528
</code></pre>

<p>The Ruby environment used here is <a href="http://rvm.io/">RVM</a>. Ruby version is 2.1.2.</p>

<pre><code>$ rvm use 2.1.2 --default
$ cat closure.rb
def extent
    n = 0
    lambda {
        n += 1
        printf "n=%d\n", n
    }
end

extent().call()
extent().call()
f = extent()
puts '------'
f.call()
f.call()
puts "class of f is #{f.class.name}, its id is #{f.object_id}"

$ ruby closure.rb
n=1
n=1
------
n=1
n=2
class of f is Proc, its id is 6724520
</code></pre>

<p>Two versions of Scala, define functions via &ldquo;def&rdquo; and &ldquo;val&rdquo; give different results:</p>

<pre><code>$ diff *.scala
1c1
&lt; def extent = {
---
&gt; val extent = {

$ cat closure-def.scala
def extent = {
    var n = 0
    () =&gt; {
        n += 1
        println("n=" + n)
    }
}

extent()
extent()
val f = extent
println("------")
f()
f()
println(s"Class of f is ${f.getClass}, its id is ${f.hashCode}")

$ scala closure-def.scala
n=1
n=1
------
n=1
n=2
Class of f is class Main$$anon$1$$anonfun$extent$1, its id is 8970973

$ cat closure-val.scala 
val extent = {
    var n = 0
    () =&gt; {
        n += 1
        println("n=" + n)
    }
}

extent()
extent()
val f = extent
println("------")
f()
f()
println(s"Class of f is ${f.getClass}, its id is ${f.hashCode}")
$ scala closure-val.scala 
n=1
n=2
------
$ scala closure-val.scala 
n=1
n=2
------
n=3
n=4
Class of f is class Main$$anon$1$$anonfun$1, its id is 8970973
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run MapReduce Jobs on Spark]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/15/175039/"/>
    <updated>2014-08-15T17:50:39+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/15/175039</id>
    <content type="html"><![CDATA[<h1>Interactive Mode</h1>

<p>Follow <a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start of Spark 1.0.2 on Apache Spark website</a>.</p>

<pre><code># hadoop fs -put alarm_data_for_explore-0501-0505.txt alarm_data_for_explore-0501-0505.txt 

# hadoop fs -ls
...
-rw-r--r--   3 root root  260780698 2014-08-15 16:45 alarm_data_for_explore-0501-0505.txt

# wc -l alarm_data_for_explore-0501-0505.txt
1362005

# head -1 alarm_data_for_explore-0501-0505.txt 
-2117657102|102|1000012276|License...|3|4|102|

# grep 007-002-00-000592 alarm_data_for_explore-0501-0505.txt | wc -l
12

# mv alarm_data_for_explore-0501-0505.txt aaa.txt       // this local file is unnecessary any more

# spark-shell

scala&gt; val textFile = sc.textFile("alarm_data_for_explore-0501-0505.txt")
textFile: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at &lt;console&gt;:12

scala&gt; textFile.count()
...
res5: Long = 1362005

scala&gt; textFile.first()
...
res8: String = -2117657102|102|1000012276|License...|3|4|102|

scala&gt; textFile.filter(line =&gt; line.contains("007-002-00-000592")).count()
...
res12: Long = 12

scala&gt; import java.lang.Math
import java.lang.Math

scala&gt; textFile.map(line =&gt; line.split("|").size).reduce((a, b) =&gt; Math.max(a, b))
...
res13: Int = 372
</code></pre>

<h2>Run Spark Script</h2>

<pre><code>$ cat wfp-spark
val MIN_SUP = 0.0003
val MIN_CONF = 0
val MAX_RELATION_ORDER = 3
val DATA_FILE = "input"

val textFile = sc.textFile(DATA_FILE)
val weight = textFile.map(x =&gt; x -&gt; x.split(",")(5).toFloat).cache
val data = textFile.groupBy(x =&gt; x.split(",")(0))

$ spark-shell -i wfp-spark
...
</code></pre>

<p>Now you are in a spark shell, all variables such as MIN_SUP, MIN_CONF are accessible.</p>

<h1>Batch Mode</h1>

<p>Follow <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_running_spark_apps.html">Running Spark Applications in CDH 5 Installation Guide</a>.</p>

<pre><code># locate spark-defaults.conf       // find out where is $SPARK_HOME
/etc/spark/conf.cloudera.spark/spark-defaults.conf
...

# ll /etc/spark/conf.cloudera.spark
...
-rw-r--r-- 1 root root 883 Aug 14 10:12 spark-env.sh

# cat /etc/spark/conf.cloudera.spark/spark-env.sh
...
export SPARK_HOME=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/spark
...

# source /etc/spark/conf.cloudera.spark/spark-env.sh     // load $SPARK_HOME, etc

# spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode client --master yarn $SPARK_HOME/examples/lib/spark-examples_2.10-1.0.0-cdh5.1.0.jar 10
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Underscore in Scala]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/13/102312/"/>
    <updated>2014-08-13T10:23:12+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/13/102312</id>
    <content type="html"><![CDATA[<p>val name = &ldquo;abc&rdquo;
println(name.exists(_.isUpper))</p>

<p>Here &ldquo;_.isUpper&rdquo; is an anoymous function, which equals to &ldquo;x => x.isUpper&rdquo;.</p>

<p>Ref:
<a href="http://stackoverflow.com/questions/8260367/scala-placeholder-syntax">Scala placeholder syntax</a>
<a href="http://docs.scala-lang.org/tutorials/tour/anonymous-function-syntax.html">Anonymous Function Syntax</a>
<a href="http://ananthakumaran.in/2010/03/29/scala-underscore-magic.html">Scala underscore magic</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Development on Linux]]></title>
    <link href="http://leetschau.github.io/blog/2014/08/07/092400/"/>
    <updated>2014-08-07T09:24:00+08:00</updated>
    <id>http://leetschau.github.io/blog/2014/08/07/092400</id>
    <content type="html"><![CDATA[<h1>sbt + vim</h1>

<ol>
<li><p>Download sbt package (sbt-0.13.5.deb for Mint 17 64bit) from <a href="http://www.scala-sbt.org/">sbt website</a>;</p></li>
<li><p>Install it, then start sbt repl with <code>sbt</code> in shell;</p></li>
</ol>


<p>This will install sbt and scala.</p>

<p>Now you can run scala repl with <code>sbt console</code> in shell, or run <code>console</code> within sbt repl.</p>

<h2>Script REPL</h2>

<p>Create a file &ldquo;Hi.scala&rdquo;:</p>

<pre><code>object Hi extends App {
    println("Hi, there")
}
</code></pre>

<p>Then run this script within sbt:</p>

<pre><code>$ sbt
...
&gt; run
...
[info] Running Hi
Hi, there
</code></pre>

<p>You can use tilde prefix to detect source change automatically, just use &ldquo;~run&rdquo; instead of &ldquo;run&rdquo; in above example.</p>
]]></content>
  </entry>
  
</feed>
