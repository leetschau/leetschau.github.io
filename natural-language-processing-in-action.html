<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Li Chao" />
        <meta name="copyright" content="Li Chao" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="NLP, Python, Tech, " />

<meta property="og:title" content="Natural Language Processing in Action"/>
<meta property="og:url" content="http://leetschau.github.io/natural-language-processing-in-action.html" />
<meta property="og:description" content="本文是 Natural Language Processing in Action by Hobson Lane, 2019 的读书笔记， 相关代码见 my fork of the official repo。 Environment Setup git clone git@github.com:slm-bj/nlpia.git cd nlpia python -m venv env . env/bin/activate pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip pip …" />
<meta property="og:site_name" content="DarkMatter in Cyberspace" />
<meta property="og:article:author" content="Li Chao" />
<meta property="og:article:published_time" content="2020-07-29T07:02:28+08:00" />
<meta property="" content="2020-08-14T08:06:53+08:00" />
<meta name="twitter:title" content="Natural Language Processing in Action">
<meta name="twitter:description" content="本文是 Natural Language Processing in Action by Hobson Lane, 2019 的读书笔记， 相关代码见 my fork of the official repo。 Environment Setup git clone git@github.com:slm-bj/nlpia.git cd nlpia python -m venv env . env/bin/activate pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip pip …">

        <title>Natural Language Processing in Action · DarkMatter in Cyberspace
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://leetschau.github.io/theme/css/custom.css" media="screen">
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="http://leetschau.github.io/"><span class=site-name>DarkMatter in Cyberspace</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="http://leetschau.github.io">Home</a></li>
                            <li ><a href="http://leetschau.github.io/categories.html">Categories</a></li>
                            <li ><a href="http://leetschau.github.io/tags.html">Tags</a></li>
                            <li ><a href="http://leetschau.github.io/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="http://leetschau.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="http://leetschau.github.io/natural-language-processing-in-action.html"> Natural Language Processing in Action </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            
            <hr>
<p>本文是 Natural Language Processing in Action by Hobson Lane, 2019
的读书笔记，
相关代码见 <a href="https://github.com/slm-bj/nlpia">my fork of the official repo</a>。</p>
<h1>Environment Setup</h1>
<div class="highlight"><pre><span></span><code><span class="err">git clone git@github.com:slm-bj/nlpia.git</span>
<span class="err">cd nlpia</span>
<span class="err">python -m venv env</span>
<span class="err">. env/bin/activate</span>
<span class="err">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip</span>
<span class="err">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -e .</span>
<span class="err">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pynvim vaderSentiment</span>
</code></pre></div>


<p><code>pip</code> 的 <code>-e</code> 选项使得 pip 以 development mode 安装本地包（这里是 nlpia），
也就是在开发路径下新建一个以 .egg-info 结尾的目录（这里是 src/nlpia.egg-info）
作为本地包的安装路径（而不是安装到 PYTHON_HOME/site-packages 下），
另外安装的包可以不完全遵循 requirements.txt 的要求，
比如 tensorflow 实际安装的版本是 2.3.0，与 requirements.txt 指定的版本不符，
并且 requirements.txt 中的 AIML-Bot, Keras-Applications 等包都没有装。</p>
<h1>Preface</h1>
<p>如果思维是用自然语言表达的，
NLP 就是实现通用人工智能 AGI 的关键步骤。</p>
<p>NLP 已经在影响社会和自我训练方面形成了闭环，发挥了很大作用，
未来的影响会越来越大。</p>
<h1>About this book</h1>
<h2>Roadmap</h2>
<p>运行代码：将数据文件（csv）放入 src/nlpia/data 目录，
然后运行 <code>nlpia.data.loaders.get_data()</code>.</p>
<p>Part 1: 使用算数方法构建基本的 NLP 算法，比如 email spam filter；</p>
<p>Part 2: 使用神经网络构建 NLP 算法；</p>
<p>Part 3: 使用前面学到的方法构造真实的聊天机器人。</p>
<h1>Chapter 1</h1>
<h2>1.4.3</h2>
<p>bag-of-words 是输入文本去掉了 stop word（例如 a, of 等）和 rare word（例如人名）
后的词频统计表，见 p18 图示和下面的 Python 代码。
可以用 Python 的 collections.Counter 对象实现。</p>
<p>使用 bag-of-words 代表一句话（比如用户的提问），
再用 bag-of-words 代表一篇文档（最能回答用户提问的文档），
寻找这两个 vector 之间的关系。</p>
<h2>1.5</h2>
<p>第一种实用的 NLP 编码方法：one-hot encoded vectors (p20)</p>
<h2>1.6</h2>
<p>第一段谈到前面的 bag-of-words 方法虽然丢弃了单词顺序，但处理比较短的文本仍然效果良好，
我觉得原因在于英语、德语这类语言对语法成分的顺序不敏感，
但对于中文这样的分析型语言，顺序是特别重要的，采用 bag-of-words 效果大概不会好。</p>
<h1>Chapter 2</h1>
<h2>2.2</h2>
<p>bag-of-words vector = word frequency vector
只包含 word frequency，不包含词的顺序，每个向量长度为词汇表长度。
见 p39.</p>
<h3>2.2.1</h3>
<p>p40:</p>
<div class="highlight"><pre><span></span><code><span class="err">df = pd.DataFrame(pd.Series(dict([(token, 1) for token in sentence.split()])), columns=[&#39;sent&#39;]).T</span>
</code></pre></div>


<p>注意这里如何用 <code>pd.DataFrame</code> 的 <code>T</code> 属性实现 dataframe 的行列对调。</p>
<p>p41:
dot product 相当于 数据库的 inner join，相当于 矩阵乘法，
一个行向量 dot product 一个列向量得到一个标量。</p>
<h3>2.2.2</h3>
<p>Listing 2.6:</p>
<p>采用 dot 计算重合度的原理是：如果其中有一个向量在某个词下频率为0，则乘积为0：</p>
<div class="highlight"><pre><span></span><code><span class="err">              sent0  sent1  sent2  sent3</span>
<span class="err">Thomas            1      0      0      0</span>
<span class="err">Jefferson         1      0      0      0</span>
<span class="err">began             1      0      0      0</span>
<span class="err">building          1      0      0      0</span>
<span class="err">Monticello        1      0      0      1</span>
<span class="err">at                1      0      0      0</span>
<span class="err">the               1      0      1      0</span>
<span class="err">age               1      0      0      0</span>
<span class="err">of                1      0      0      0</span>
<span class="err">26.               1      0      0      0</span>
<span class="err">Construction      0      1      0      0</span>
<span class="err">was               0      1      0      1</span>
<span class="err">done              0      1      0      0</span>
<span class="err">mostly            0      1      0      0</span>
<span class="err">by                0      1      0      0</span>
<span class="err">local             0      1      0      0</span>
<span class="err">masons            0      1      0      0</span>
<span class="err">and               0      1      0      0</span>
<span class="err">carpenters.       0      1      0      0</span>
<span class="err">He                0      0      1      0</span>
<span class="err">moved             0      0      1      0</span>
<span class="err">into              0      0      1      1</span>
<span class="err">South             0      0      1      0</span>
<span class="err">Pavilion          0      0      1      0</span>
<span class="err">in                0      0      1      0</span>
<span class="err">1770.             0      0      1      0</span>
<span class="err">Turning           0      0      0      1</span>
<span class="err">a                 0      0      0      1</span>
<span class="err">neoclassical      0      0      0      1</span>
<span class="err">masterpiece       0      0      0      1</span>
<span class="err">Jefferson&#39;s       0      0      0      1</span>
<span class="err">obsession         0      0      0      1</span>
</code></pre></div>


<h3>2.2.3</h3>
<p>p49.
n-gram: 长度为n的词组，这里的词指的是空格分开的连续英文字母。
例如 "ice cream" 是 2-gram，"beyond the pale" 是 3-gram。</p>
<p>n-gram 可以有效的解决 tokenize 丢失词序导致的意思错误，
比如 "was not" 是一个 2-gram，整体作为一个 token，
比单个的 "was"、"not" 含义更准确。</p>
<p>p52, Stop Words</p>
<p>当资源重复，词汇表又比较大时，不要忽略 stop words.
由于 NLTK 的 stop words 列表随版本变化，
忽略 stop words 可能会导致程序运行结果随 NLTK 版本变化而变化。</p>
<h3>2.2.5</h3>
<p>缩小词汇表有助于降低 overfitting 的可能性。</p>
<h4>Case Folding</h4>
<p>将不同大小写的单词当成相同的单词。</p>
<p>最简单的方法：全部转小写。</p>
<p>会丢失信息，例如 <code>Doctor</code> 和 <code>doctor</code>。</p>
<p>可以只转换句首的大写字母，</p>
<p>仍然会丢失信息，要在效率和准确性之间权衡。</p>
<h4>Stemming</h4>
<p>去掉名词复数、所有格 (possessive words)、动行变形（时态等）产生的变化。</p>
<p>p58（拷贝自 2020.7.15 日记）:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">re</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;^(.*ss|.*?)(s)?$&#39;</span><span class="p">,</span> <span class="s2">&quot;housees&quot;</span><span class="p">)</span>
<span class="p">[(</span><span class="s1">&#39;housee&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">)]</span>
</code></pre></div>


<p><code>|</code> 的优先级比较低，所以正则表达式的意思是：
要么匹配 <code>ss</code> 结尾，要么以 non-greedy 方式匹配任何单词。</p>
<p>在 <code>.*ss</code> 情形下，由于采用贪婪模式，即使有多于两个的 s，
也仍然匹配最后的两个 s，所以第二组 <code>(s)?</code> 总为空。</p>
<p>如果不是以两个或两个以上 s 结尾，则由于采用 non-greedy 模式（星号后面的问好将星号转为非贪婪模式），
第二组能匹配则尽量匹配，也就是说，只要是一个 s 结尾，则一定被分配到第二组。</p>
<p>p59：Porter 一生心血打造的 stemmer 算法，就在那 300 行 Python 代码里，
让人好生感慨。</p>
<h4>Lemmatization</h4>
<p>lemma: 词根</p>
<p>Lemmatization 也会导致单词含义被改变，从而造成分析失真。</p>
<p>Lemmatizer 在 NLP pipeline 中的位置比 stmmer 靠前，
因为 lemmatizer 处理后的 token 仍然是正常的单词，
可以被 stemmer 使用。</p>
<p>POS: part of speech，大致相当于一个单词在句子中的角色，或者词性，
比如是名词、形容词还是动词。</p>
<p>p61: Princeton WordNet 居然连 best 和 good 的联系都没有，实在有失水准。</p>
<h4>Use Case</h4>
<p>是否使用 normalization 技术要随算法目标而定。
信息搜索场景中，如果需要找到尽可能多的结果，容忍一定的假阳性，则使用 normalization，
而聊天机器人对准确性的要求更高，所以一般不用 normalization。</p>
<p>Bottom line：除非特殊情况（比如研究对象是充满专业术语的论文等），
不要使用 normalization。</p>
<h2>2.3</h2>
<p>计算机技术中，heuristic 是人类通过经验编写的 if-else 式的算法，
与之相对的是基于机器学习，或者数学公式的算法。</p>
<p>Colins Cobuild 的解释是：</p>
<blockquote>
<p>A heuristic computer program uses rules based on previous experience in order to solve a problem,
rather than using a mathematical procedure.</p>
</blockquote>
<p>Vader 使用 heuristic 方法做 sentiment 分析。</p>
<p>Naive Bayes 是有监督学习，首先基于一个打好标记的语料库上建立模型。</p>
<h3>2.3.2</h3>
<p>p67:</p>
<div class="highlight"><pre><span></span><code><span class="err">movies[&#39;predicted_sentiment&#39;] = nb.predict_proba(df_bows) * 8 - 4</span>
</code></pre></div>


<p>应改为</p>
<div class="highlight"><pre><span></span><code><span class="err">movies[&#39;predicted_sentiment&#39;] = nb.predict_proba(df_bows)[:, 1] * 8 - 4</span>
</code></pre></div>


<p><code>nb.predict_proba(df_bows)</code> 的计算结果是一个形状为 (10605, 2) 的 numpy array,
行数与 movies 的观测数相等，每行包含两个值，分别是为 0 （代表文档有否定倾向）
和 为 1 （代表文档有肯定倾向）的概率，和为 1（没有其他情况）。
所以这里只取每个文档肯定倾向的概率作为 movies 的预测值。</p>
<p>赞同的概率再 <code>* 8 - 4</code> 的原因是赞同概率原来分布范围是 [0, 1]，
但 movies.sentiment 分布范围是 [-4, 4]（基于 <code>movies.sentiment.describe()</code>）。</p>
<p>pandas.DataFrame.append() 可以将两个列不一样的 dataframe 以并集的方式连在一起：</p>
<div class="highlight"><pre><span></span><code><span class="k">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">d1</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="err">{</span><span class="ss">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="ss">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span><span class="err">}</span><span class="p">)</span>

<span class="k">In</span> <span class="p">[</span><span class="mi">12</span><span class="p">]:</span> <span class="n">d2</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="err">{</span><span class="ss">&quot;c&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="ss">&quot;d&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">]</span><span class="err">}</span><span class="p">)</span>

<span class="k">In</span> <span class="p">[</span><span class="mi">13</span><span class="p">]:</span> <span class="n">d1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">d2</span><span class="p">)</span>
<span class="k">Out</span><span class="p">[</span><span class="mi">13</span><span class="p">]:</span>
     <span class="n">a</span>    <span class="n">b</span>     <span class="k">c</span>     <span class="n">d</span>
<span class="mi">0</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0</span>  <span class="mi">3</span><span class="p">.</span><span class="mi">0</span>   <span class="n">NaN</span>   <span class="n">NaN</span>
<span class="mi">1</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">0</span>  <span class="mi">4</span><span class="p">.</span><span class="mi">0</span>   <span class="n">NaN</span>   <span class="n">NaN</span>
<span class="mi">0</span>  <span class="n">NaN</span>  <span class="n">NaN</span>  <span class="mi">11</span><span class="p">.</span><span class="mi">0</span>  <span class="mi">13</span><span class="p">.</span><span class="mi">0</span>
<span class="mi">1</span>  <span class="n">NaN</span>  <span class="n">NaN</span>  <span class="mi">12</span><span class="p">.</span><span class="mi">0</span>  <span class="mi">14</span><span class="p">.</span><span class="mi">0</span>
<span class="mi">2</span>  <span class="n">NaN</span>  <span class="n">NaN</span>   <span class="mi">5</span><span class="p">.</span><span class="mi">0</span>  <span class="mi">18</span><span class="p">.</span><span class="mi">0</span>

<span class="k">In</span> <span class="p">[</span><span class="mi">14</span><span class="p">]:</span> <span class="n">d3</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="err">{</span><span class="ss">&quot;c&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="ss">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">]</span><span class="err">}</span><span class="p">)</span>
<span class="k">In</span> <span class="p">[</span><span class="mi">16</span><span class="p">]:</span> <span class="n">d1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">d3</span><span class="p">)</span>
<span class="k">Out</span><span class="p">[</span><span class="mi">16</span><span class="p">]:</span>
     <span class="n">a</span>   <span class="n">b</span>     <span class="k">c</span>
<span class="mi">0</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0</span>   <span class="mi">3</span>   <span class="n">NaN</span>
<span class="mi">1</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">0</span>   <span class="mi">4</span>   <span class="n">NaN</span>
<span class="mi">0</span>  <span class="n">NaN</span>  <span class="mi">13</span>  <span class="mi">11</span><span class="p">.</span><span class="mi">0</span>
<span class="mi">1</span>  <span class="n">NaN</span>  <span class="mi">14</span>  <span class="mi">12</span><span class="p">.</span><span class="mi">0</span>
<span class="mi">2</span>  <span class="n">NaN</span>  <span class="mi">18</span>   <span class="mi">5</span><span class="p">.</span><span class="mi">0</span>

<span class="k">In</span> <span class="p">[</span><span class="mi">20</span><span class="p">]:</span> <span class="n">df_bows</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">df_pbows</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_all_bows</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">Out</span><span class="p">[</span><span class="mi">20</span><span class="p">]:</span> <span class="mi">3141</span>
<span class="k">In</span> <span class="p">[</span><span class="mi">48</span><span class="p">]:</span> <span class="n">df_pbows</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3141</span>
<span class="k">Out</span><span class="p">[</span><span class="mi">48</span><span class="p">]:</span> <span class="mi">2546</span>
</code></pre></div>


<p>表明 <code>df_bows</code> 和 <code>df_pbows</code> 中有 3141 个 token 是重合的，
从 products 总词汇表容量为 <code>df_pbows.shape[1]</code>，
去掉重合的 3141 个，剩下正好是书中所说的，有 2546 个 token 不在 movies 的词汇表里，
被下面的操作去掉了：</p>
<div class="highlight"><pre><span></span><code><span class="err">df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]</span>
</code></pre></div>


<p>改成在 products 上拟合，而不是使用 movies 上的拟合模型 nb：</p>
<div class="highlight"><pre><span></span><code><span class="n">products</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="s1">&#39;hutto_products&#39;</span><span class="p">)</span>
<span class="n">bows</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="nb">text</span> <span class="k">in</span> <span class="n">products</span><span class="p">.</span><span class="nb">text</span><span class="p">:</span>
    <span class="n">bows</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">Counter</span><span class="p">(</span><span class="n">casual_tokenize</span><span class="p">(</span><span class="nb">text</span><span class="p">)))</span>

<span class="n">df_pbows</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">bows</span><span class="p">)</span>
<span class="n">df_pbows</span> <span class="o">=</span> <span class="n">df_pbows</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">nbp</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">df_pbows</span><span class="p">,</span> <span class="n">products</span><span class="p">.</span><span class="n">sentiment</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="o">#</span> <span class="err">在</span> <span class="n">products</span> <span class="err">上拟合</span>
<span class="n">products</span><span class="p">[</span><span class="s1">&#39;predicted_sentiment&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nbp</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">df_pbows</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">-</span> <span class="mi">4</span>
<span class="n">products</span><span class="p">[</span><span class="s1">&#39;sentiment_ispositive&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">products</span><span class="p">.</span><span class="n">sentiment</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">products</span><span class="p">[</span><span class="s1">&#39;predicted_ispositive&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">products</span><span class="p">.</span><span class="n">predicted_sentiment</span>
                                    <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="p">(</span><span class="n">products</span><span class="p">.</span><span class="n">predicted_ispositive</span> <span class="o">==</span>
 <span class="n">products</span><span class="p">.</span><span class="n">sentiment_ispositive</span><span class="p">).</span><span class="k">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">len</span><span class="p">(</span><span class="n">products</span><span class="p">)</span>
</code></pre></div>


<p>准确率 0.8847，远好于示例代码 0.5572 的准确率。</p>
<h1>Chapter 3</h1>
<p>本章讲如何用数值表示一个 token 的重要性，使用的三种方法都是基于频率的。
第 4 章讲如何用数值表示一个 token 的含义。</p>
<h2>3.1</h2>
<p>上一章 one-hot 编码中，dataframe 的结构是：
每行代表一个文档（例如一条twitter，或者一条电影评论），
没列是一个词，每个 cell 代表当前词（列）在当前文档（行）中出现的次数。</p>
<p>term frequency, TF: the number of times a word occurs in a document.</p>
<p>p74 代码：</p>
<div class="highlight"><pre><span></span><code><span class="err">tf = times_harry_appears / num_unique_words</span>
</code></pre></div>


<p>对单词 <code>harry</code> 的 TF 做了 normalization，也就是除以文档总词汇量。</p>
<p><code>A is tempered by B</code> 表示 A 被 B 温和化，去极端化，
这里用 A 除以 B，从而避免 B 不同时，只用 A 比较导致的不准确。</p>
<h2>3.2</h2>
<p>Lexicon: 词汇表，所有 doc 中词汇的并集。</p>
<p>p77 代码分析：</p>
<p><code>doc_tokens</code> 是一个元素为 list （对应一个 doc，元素是 token，即单词）的 list，
<code>all_doc_tokens = sum(doc_tokens, [])</code> 将嵌套结构打平，
不改变顺序直接连在一起。</p>
<p><code>lexicon</code> 是一个排序后长度为 18 的 list，代表 3 个文档组成的完整单词表。</p>
<p><code>zero_vector</code> 是一个 lexicon 中所有 token 对应 TF 全为 0 的字典，
后面每轮循环处理一个 doc，在 zero_vector 中，当前文档包含的单词上更新 TF，
而没有被更新到的 token（lexicon 里有，但当前文档里没有的 token）仍然保持 TF 为 0。</p>
<h3>3.2.1</h3>
<p>一般用 K 表示 lexicon 的大小，有些论文里用 <code>[V]</code> 表示。
上面 p77 3 个文档的例子里，<code>K = len(lexicon) = 18</code>。</p>
<p>Fig 3.2 和 Fig 3.3 中图例标注错误，应为 doc_0, doc_1, doc_2，
而不是两个 doc_0 和一个 doc_2。</p>
<p>p82 第2行 Python 表达式有误，应为
<code>a.dot(b) = norm(a) * norm(b) * cos(theta)</code> 而非
<code>a.dot(b) = norm(a) * norm(b) / cos(theta)</code>。</p>
<p>下面这句话成立的前提是 A 和 B 长度相同（p82 倒数第2段倒数第2句话）：</p>
<blockquote>
<p>A、B （假设 A 比 B 长）夹角的 cos 值，相当于 B 在 A 上的投影长度与 A 长度的比值。</p>
</blockquote>
<p>如果两个 doc 的 TF cos 相似度为 1，说明二者使用相同的词，且这选词在文档中的比例相同，
从而证明这两个文档高度类似。
cos 相似度为 0 不一定表明两个文档讨论的不是一个话题，
只是表示两个文档使用的词完全不同。
由于词出现在文档中的频率不可能是负数，导致 cos 相似度最小为 0，而不会小于 0。</p>
<h2>3.3</h2>
<p>Zipf（p 不发音）定律：一篇文档，按单词出现频率对所有单词排序，比如 "the" 排第一，"a" 排第二，
"of" 排第三，则每个单词在整个文档中所占的比重是其排名的倒数。
排名第一的作为标杆（1 的倒数还是 1），第二名（"a"）出现的次数是它（"the"）的 1/2，
第三名是它的 1/3，等等。</p>
<p>详见 <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf's law</a> 第2段的例子。</p>
<h3>获取 nltk_data 方法</h3>
<p>nltk 使用 <code>nltk.data.path</code> 寻找数据位置，这是一个标准的 Python string list，
目录最后必须是 nltk_data，例如 ~/Documents/machineLearningDatasets/nltk_data，
将原始 brown.zip 文件保存到 .../nltk_data/corpora 下，
并在这个目录下解压 zip 文件，得到 .../nltk_data/corpora/brown 目录，
下面包括许多文本文件。</p>
<p>下载原始数据：</p>
<div class="highlight"><pre><span></span><code><span class="err">git clone https://github.com/nltk/nltk_data.git</span>
</code></pre></div>


<p>zip 文件在 nltk_data/packages/corpora 下。</p>
<p>使用方法：在需要使用 nltk_data 的 Python 代码里添加：</p>
<div class="highlight"><pre><span></span><code><span class="err">nltk.data.path.append(&#39;/home/leo/Documents/machineLearningDatasets/nltk_data&#39;)</span>
</code></pre></div>


<p>然后就可以正常使用了：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>
</code></pre></div>


<h2>3.4</h2>
<p>IDF</p>
            
            
            <hr/>
            <script src="https://utteranc.es/client.js"
                    repo="leetschau/leetschau.github.io"
                    issue-term="pathname"
                    theme="github-light"
                    crossorigin="anonymous"
                    async>
            </script>
            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2020-07-29T07:02:28+08:00">Jul 29, 2020</time>

<h4>Last Updated</h4>
<time datetime="2020-08-14T08:06:53+08:00">Aug 14, 2020</time>

            <h4>Category</h4>
            <a class="category-link" href="http://leetschau.github.io/categories.html#tech-ref">Tech</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="http://leetschau.github.io/tags.html#nlp-ref">NLP
                    <span>1</span>
</a></li>
                <li><a href="http://leetschau.github.io/tags.html#python-ref">Python
                    <span>136</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="#" title="My You can add links in your config file Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-you can add links in your config file sidebar-social-links"></i></a>
    <a href="#" title="My Another social link Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-another social link sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>